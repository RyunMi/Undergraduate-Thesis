# 虚拟对抗训练：监督和半监督学习的正则化方法

## 引用方式

### GB/T 7714

<span style="background-color: rgb(255, 255, 255)">Miyato T, Maeda S, Koyama M, et al. Virtual adversarial training: a regularization method for supervised and semi-supervised learning[J]. IEEE transactions on pattern analysis and machine intelligence, 2018, 41(8): 1979-1993.</span>

### BibTex

    @article{miyato2018virtual,
      title={Virtual adversarial training: a regularization method for supervised and semi-supervised learning},
      author={Miyato, Takeru and Maeda, Shin-ichi and Koyama, Masanori and Ishii, Shin},
      journal={IEEE transactions on pattern analysis and machine intelligence},
      volume={41},
      number={8},
      pages={1979--1993},
      year={2018},
      publisher={IEEE}
    }

## 开源代码

[lyakaap/VAT-pytorch: Virtual Adversarial Training (VAT) implementation for PyTorch (github.com)](https://github.com/lyakaap/VAT-pytorch)

## 主要思想

本文提出了一种基于虚拟对抗损失的新的正则化方法：一种新的给定输入的条件标签分布的局部平滑度量。

虚拟对抗损失被定义为每个输入数据点周围的条件标签分布对局部扰动的鲁棒性。与对抗训练不同，本文的方法不需要标签信息来定义对抗方向，因此适用于半监督学习。

VAT的计算成本相对较低。对于神经网络，虚拟对抗损失的近似梯度只需进行两次前向和后向传播即可计算。

在本文的实验中，将VAT应用于多个基准数据集上的监督和半监督学习任务。通过基于最小化熵原理的简单算法改进，本文的VAT在SVHN和CIFAR-10的半监督学习任务中取得了最先进的性能。

## 主要内容

### 引论

在实际回归和分类问题中，人们必须面对两个相对的问题：欠拟合和过拟合。正则化是引入更多信息的过程，以便管理训练误差和测试误差之间不可避免的差距降低以过拟合，本文介绍一种新的正则化方法，适用于半监督学习，该方法确定分类器行为最敏感的方向。

从贝叶斯的角度来看，正则项可以解释为先验分布，反映了关于模型的先验知识或信念。一个广为人知的先验信念是，基于广泛观察到的事实，大多数自然形成的系统的输出在空间和时间输入方面都很平滑。当构建概率模型时，这种信念促使研究者更喜欢相对于条件输入 $x$ 平滑的条件输出分布 $p(y|x)$ 。

在实践中，将输出分布进行平滑常常有利。例如，标签传播算法通过根据信念将类标签分配给未标记的训练样本，改善分类器的性能，即相似的输入数据点倾向于具有相似的类标签。

此外对于神经网络，可以通过对每个输入应用随机扰动来提高泛化性能，以生成人工输入点并促使模型将类似的输出分配给从同一点派生的人工输入集。在半监督学习中，使预测器抵抗随机和局部扰动的理念是有效的。

然而，前述理论在简单应用上会有缺陷：通过随机噪声和随机数据增强的标准**各向同性**平滑通常会使预测器在特定方向(即对抗方向)上特别容易受到小扰动的影响，这个方向是在输入空间中标签概率 $p(y=k|x)$ 最为敏感的方向。使用标准正则化技术(如 $L_1$ 和 $L_2$ 正则化)训练的预测器在信号向对抗方向扰动时很可能会犯错，即使扰动的规范很小，人眼无法察觉。

受这一发现的启发，Goodfellow开发了对抗性训练，训练模型给每个输入数据分配标签，其类似于其对抗方向邻居应分配的标签。这次尝试成功地提高了泛化性能，并使模型能够抵御对抗性扰动的影响。

本文提出一种正则化技术，通过选择性地在其最非各向同性上平滑模型，来训练*输出分布*在每个输入数据点周围呈各向同性平滑的方法。

为量化此想法，引入了虚拟对抗方向的概念，其是扰动的方向，可以在分布差异的意义上最大程度地改变输出分布。虚拟对抗方向是本文对“最”各向异性方向的解释。

由Goodfellow等人引入的对抗性方向是一个扰动的方向，它可以最大程度地减少该模型正确分类的概率，或者是可以最大程度地“偏离”模型对正确标签的预测的方向。与对抗性方向不同，虚拟对抗方向可以在未标记的数据点上定义(就像存在“虚拟”标签一样)，因其是可以最大程度地偏离当前推断输出分布的方向。

优点：

1.适用于半监督学习任务

2.适用于任何参数化模型，只要能够根据输入和参数计算出梯度。

算法似乎需要解决一个内部优化问题才能确定虚拟对抗方向。对于像NN这样的模型，可以评估其输出相对于输入的梯度，但是虚拟对抗扰动却采用了一个可以使用power方法高效计算的近似值。这是 VAT 算法的重要部分，让它可以轻松应用于各种设置和模型体系结构。

3.少量超参数

4.参数化不变的正则化

这是VAT与流行的正则化方法如 $L_p$ 正则化有所不同的最基本的点。对于线性模型， $L_p$ 正则化的效果是减轻输出对输入的过度敏感，并且可以通过超参数来控制其影响的强度。当所关注的模型非常非线性，例如神经网络的情况下， $L_p$ 正则化的效果很难被用户控制。

在贝叶斯统计学的语言中，将正则化项解释为先验分布。这就是说， $L_p$ 正则化所偏爱的先验分布的特性取决于当前的参数设置，因此模棱两可且难以评估。而参数不变正则化不存在这样的问题。

VAT是参数不变正则化，因为它通过输出对输入的局部敏感性直接正则化输出分布，这是根据定义独立于对模型进行参数化的方式。

### 方法

$x\in R^I$ 和 $y\in Q$ 分别表示输入向量和输出标签，其中 $I$ 是输入维度， $Q$ 是所有标签的空间。

用 $\theta$ 参数化的输出分布表示为 $p(y|x,\theta)$ ，使用 $\hat{\theta}$ 来表示训练过程中特定迭代步骤的模型参数向量。

使用 $\mathcal{D}\_l=\{x_l^{(n)},y_l^{(n)}|n=1,\dots,N_l\}$ 表示标签数据集，使用 $\mathcal{D}\_{ul}=\{x_{ul}^{m)}|m=1,\dots,N_{ul}\}$ 表示无标签数据集。利用 $\mathcal{D}\_l$ 和 $\mathcal{D}\_{ul}$ 来训练模型 $p(y|x,\theta)$ 。

#### 对抗训练

介绍方法之前制定对抗性训练，对抗训练的损失函数可以写为：

![\<img alt="" data-attachment-key="NR58UZ3M" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F2UA8823C%22%2C%22annotationKey%22%3A%22YUCA97DD%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B321.215%2C186.762%2C533.105%2C223.956%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%223%22%7D%7D" width="353" height="62" src="attachments/NR58UZ3M.png" ztype="zimage">](attachments/NR58UZ3M.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/Z5MLKUYC">Miyato 等, 2018, p. 3</a></span>)</span>

其中， $D[p,p′]$ 是一个非负函数，用于测量两个分布 $p$ 和 $p′$ 之间的差异。 $D$ 可以是交叉熵 $D[p,p′]=-\sum_i p_i \log p′_i$ ，其中 $p$ 和 $p′$ 是向量，它们的第 $i$ 个坐标表示第 $i$ 个类别的概率。函数 $q(y|x_l)$ 是输出标签的真实分布(未知)。

上图损失函数的目标是通过一个对 $x$ 的对抗攻击具有鲁棒性的参数模型 $p(y|x_l,\theta)$ 来近似真实分布 $q(y|x_l)$ 。

在Goodfellow的研究中，函数 $q(y|x_l)$ 被独热向量 $h(y;y_l)$ (其元素除了对应于真实标签\[输出] $y_l$ 的索引外都为零)逼近。同样，对于回归任务，可以使用以 $y_l$ 为中心具有恒定方差的正态分布，或带有原子 $y=y_l$ 的 delta 函数。

通常无法获得精确对抗扰动 $r_{adv}$ 的闭合形式。在上图第二个方程中，使用 $D$ 对 $r$ 线性近似。当正则为 $L_2$ 时，对抗扰动可以近似于：

![\<img alt="" data-attachment-key="FJZNMDFR" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F2UA8823C%22%2C%22annotationKey%22%3A%224J9MCQPM%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B54.099%2C639.282%2C287.967%2C662.95%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%224%22%7D%7D" width="390" height="39" src="attachments/FJZNMDFR.png" ztype="zimage">](attachments/FJZNMDFR.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%224%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/Z5MLKUYC">Miyato 等, 2018, p. 4</a></span>)</span>

当规范为 $L_{\infin}$ 时，对抗扰动可以近似表示为：

![\<img alt="" data-attachment-key="QI2VDQ8J" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F2UA8823C%22%2C%22annotationKey%22%3A%22ZIJJ4B9R%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B129.61325966850828%2C591.2685100070679%2C214.1436464088397%2C606.4839796203275%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%224%22%7D%7D" width="141" height="25" src="attachments/QI2VDQ8J.png" ztype="zimage">](attachments/QI2VDQ8J.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%224%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/Z5MLKUYC">Miyato 等, 2018, p. 4</a></span>)</span>

对于NN来说，梯度 $g$ 可以通过反向传播有效计算。通过优化基于对抗扰动的对抗训练中的损失函数，能够训练出比使用随机扰动训练的模型有更好泛化性能的模型。

#### 虚拟对抗训练

让 $x_*$ 代表 $x_l$ 或 $x_{ul}$ ，目标函数现在为：

![\<img alt="" data-attachment-key="RU4KMP8V" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F2UA8823C%22%2C%22annotationKey%22%3A%22S5YAA9VG%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B62.552%2C364.276%2C280.077%2C405.414%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%224%22%7D%7D" width="363" height="69" src="attachments/RU4KMP8V.png" ztype="zimage">](attachments/RU4KMP8V.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%224%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/Z5MLKUYC">Miyato 等, 2018, p. 4</a></span>)</span>

实际上，没有关于 $q(y|x_{ul})$ 的直接信息。因此采取的策略是用当前近似值 $p(y|x,\theta)$ 替换 $q(y|x)$ 。当有大量带标签的训练样本时， $p(y|x,\theta)$  应该接近于 $q(y|x)$ 。所以使用从 $p(y|x,\theta)$ 概率生成的**虚拟**标签代替用户未知的标签，然后根据虚拟标签计算对抗方向。

因此本文使用当前的估计值 $p(y|x,\hat{\theta})$ 代替 $q(y|x)$ 。通过这种妥协，得到了根据对抗训练损失函数的诠释：

![\<img alt="" data-attachment-key="ZG72IB26" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F2UA8823C%22%2C%22annotationKey%22%3A%22QK3CTXTR%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B58.608%2C161.967%2C264.862%2C206.59890190672488%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%224%22%7D%7D" width="344" height="74" src="attachments/ZG72IB26.png" ztype="zimage">](attachments/ZG72IB26.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%224%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/Z5MLKUYC">Miyato 等, 2018, p. 4</a></span>)</span>

这定义了虚拟对抗扰动。 $LDS(x,\theta)$ 的损失可以视为当前模型在每个输入数据点 $x$ 处局部光滑性的负测量，它的减少将使模型在每个数据点处光滑。

本研究提出的正则化项是在所有输入数据点上 $LDS(x_*,\theta)$ 的平均值：

![\<img alt="" data-attachment-key="DEJXG5FL" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F2UA8823C%22%2C%22annotationKey%22%3A%22HQB9YJ24%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B51.84530386740331%2C40.24300000000004%2C273.878%2C73.153604918422%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%224%22%7D%7D" width="370" height="55" src="attachments/DEJXG5FL.png" ztype="zimage">](attachments/DEJXG5FL.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%224%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/Z5MLKUYC">Miyato 等, 2018, p. 4</a></span>)</span>

完整的目标函数：

![\<img alt="" data-attachment-key="4HRJEL2K" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F2UA8823C%22%2C%22annotationKey%22%3A%22VVIYS7N8%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B373.624%2C713.669%2C499.293%2C732.265%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%224%22%7D%7D" width="209" height="31" src="attachments/4HRJEL2K.png" ztype="zimage">](attachments/4HRJEL2K.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%224%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/Z5MLKUYC">Miyato 等, 2018, p. 4</a></span>)</span>

其中 $\mathcal{l}(\mathcal{D}\_l,\theta)$ 表示已标记数据集负对数似然，VAT 是一种带有正则化项 $\mathcal{R}\_{vadv}$ 的训练方法。

VAT的一个显著优点是，只有两个标量超参数：

(1)对于对抗性方向的范数约束 $\epsilon>0$ 。

(2)正则化系数 $\alpha>0$ 。

对于许多基于生成模型的监督和半监督学习方法，旨在学习 $p(y,x)$ ，训练的瓶颈是优化生成模型的超参数的困难(即 $p(x)$ 或 $p(x|y)$ 的优化)。

与对抗训练不同的是，虚拟对抗扰动的定义仅需要输入 $x$ ，而不需要标签 $y$ ，这就是能够将VAT应用于半监督学习的特性。

下图展示了在二维综合数据集上使用VAT进行半监督学习的工作原理，使用了一个拥有50个隐藏单元的一个隐藏层的NN分类器：

![\<img alt="" data-attachment-key="I6EJQPTU" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F2UA8823C%22%2C%22annotationKey%22%3A%22AVE7BTEB%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B82.276%2C518.685%2C526.343%2C748.044%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%225%22%7D%7D" width="740" height="382" src="attachments/I6EJQPTU.png" ztype="zimage">](attachments/I6EJQPTU.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%225%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/Z5MLKUYC">Miyato 等, 2018, p. 5</a></span>)</span>

在训练开始时，分类器预测同一簇中输入数据点的标签不同，并且边界处的 $LDS$ 非常高(第二列)。

本算法施加强压力，使模型在具有大 $LDS$ 值的点周围变得平滑。随着训练的进行，模型逐渐演化，以至于在 $LDS$ 值大的点上的标签预测会受到周围标记输入的强烈影响。这激励着模型预测属于同一聚类的点集具有相同的标签，这也是在半监督学习中经常希望达到的目标。

#### 快速逼近虚拟对抗扰动和目标函数的导数的方法

一旦计算出虚拟对抗扰动 $r_{vadv}$ ， $LDS(x_{\*},\theta)$ 的评估就变成了计算输出分布 $p(y|x_{\*},\hat{\theta})$ 与 $p(y|x_{\*}+r_{vadv},\theta)$ 之间的分歧 $D$ 。然而，与原始对抗训练中的线性逼近不同， $r_{vadv}$ 的评估不能使用 $D[p(y|x_{\*},\hat{\theta}),p(y|x_{\*}+r_{vadv},\hat{\theta})]$ 关于 $r$ 的梯度在 $r = 0$ 处总是为0。

为简单起见，将 $D[p(y|x_{\*},\hat{\theta}),p(y|x_{\*}+r_{vadv},\theta)]$ 表示为 $D(r,x_{\*},\theta)$ 。假设在几乎所有情况下， $p(y|x_{\*},\theta)$ 相对于 $\theta$ 和 $x$ 是可二次微分的。

由于当 $r = 0$ 时， $D(r,x_{\*},\hat{\theta})$ 取得最小值，所以可导性假设表明它的一阶导数 $\nabla_r D(r,x_{\*},\hat{\theta})|\_{r=0}$ 为零。因此， $D$ 的二阶泰勒逼近是：

![\<img alt="" data-attachment-key="CEKZQIH3" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F2UA8823C%22%2C%22annotationKey%22%3A%2247FGHJES%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B112.144%2C328.21%2C234.99399999999997%2C355.14695444686635%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%225%22%7D%7D" width="205" height="45" src="attachments/CEKZQIH3.png" ztype="zimage">](attachments/CEKZQIH3.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%225%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/Z5MLKUYC">Miyato 等, 2018, p. 5</a></span>)</span>

其中， $H(x,\hat{\theta})$ 是由 $H(x,\hat{\theta}):=\nabla\nabla_r D(r,x_{\*},\hat{\theta})|\_{r=0}$ 给出的海森矩阵。在这种近似情况下， $r_{vadv}$ 出现为 $H(x,\hat{\theta})$ 的第一个主特征向量 $u(x,\hat{\theta})$ (大小为 $\epsilon$ )：

![\<img alt="" data-attachment-key="MJD8D6JD" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F2UA8823C%22%2C%22annotationKey%22%3A%22JRFNKY2D%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B80.022%2C226.77300000000005%2C267.116%2C261.59999312089957%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%225%22%7D%7D" width="312" height="58" src="attachments/MJD8D6JD.png" ztype="zimage">](attachments/MJD8D6JD.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%225%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/Z5MLKUYC">Miyato 等, 2018, p. 5</a></span>)</span>

其中 $\overline{u}$ 表示的是和它所表示的向量 $u$ 方向相同的单位向量，即 $\overline{u}\equiv \frac{v}{||v||_2}$ 。此后为了简单起见，将 $H(x,\hat{\theta})$ 简称为 $H$ 。

接下来，需要解决计算 Hessian 矩阵的特征向量所需的 $O(I^3)$ 运行时间的问题，本文通过幂迭代方法和有限差分方法近似解决。

设 $d$ 为一个随机采样的单位向量，假如 $d$ 不垂直于主特征向量 $u$ ，迭代计算 $d\leftarrow\overline{Hd}$ ，使 $d$ 收敛于 $u$ 。为了减少计算时间，在不进行 $H$ 直接计算的情况下执行此操作。 $Hd$ 可以使用有限差分法来近似：

![\<img alt="" data-attachment-key="PTKKMLY9" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F2UA8823C%22%2C%22annotationKey%22%3A%222S63B9LM%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B335.304%2C298.906%2C540.431%2C356.387%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%225%22%7D%7D" width="342" height="96" src="attachments/PTKKMLY9.png" ztype="zimage">](attachments/PTKKMLY9.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%225%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/Z5MLKUYC">Miyato 等, 2018, p. 5</a></span>)</span>

综上，可以通过反复应用以下更新来近似 $r_{vadv}$ ：

![\<img alt="" data-attachment-key="AMN9YLV2" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F2UA8823C%22%2C%22annotationKey%22%3A%22M9QDFU97%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B378.696%2C220.011%2C477.878%2C243.116%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%225%22%7D%7D" width="165" height="38" src="attachments/AMN9YLV2.png" ztype="zimage">](attachments/AMN9YLV2.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%225%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/Z5MLKUYC">Miyato 等, 2018, p. 5</a></span>)</span>

$\nabla_r D$ 的计算可以直接进行，对于神经网络而言，这可以通过一组反向传播来实现。

这里引入的近似可以通过增加幂迭代 $K$ 的数量来单调地提高。因此对于神经网络而言， $r_{vadv}$ 的计算可以通过 $K$ 组反向传播来执行。

令人惊讶的是，在各种基准数据集上，只需要一次幂迭代就足以实现高性能。将 $K = 1$ 用于 $r_{vadv}$ 的近似会得到与前文近似 $r_{adv}$ 类似的近似值：

![\<img alt="" data-attachment-key="YRKUDZWE" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F2UA8823C%22%2C%22annotationKey%22%3A%22Q6FC9H76%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B329.105%2C44.188%2C529.724%2C92.088%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%225%22%7D%7D" width="334" height="80" src="attachments/YRKUDZWE.png" ztype="zimage">](attachments/YRKUDZWE.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%225%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/Z5MLKUYC">Miyato 等, 2018, p. 5</a></span>)</span>

在计算 $r_{vadv}$ 之后，可以在神经网络上进行一次前向和反向传播来轻松计算 $\mathcal{R}_{vadv}$ 的导数。

同时， $r_{vadv}$ 相对于 $\theta$ 的导数不仅复杂且计算成本高昂，还会引入梯度的另一个方差来源，对算法的性能产生负面影响，因此，VAT忽略了 $r_{vadv}$ 对 $\theta$ 的依赖性。

总之，包括对数似然项的完整目标函数的导数可以使用 $K+2$ 个反向传播集计算，下图总结了使用一个幂迭代近似 $\nabla_{\theta}\mathcal{R}_{vadv}$ 的小批量SGD过程。

![\<img alt="" data-attachment-key="CFCJ7BJ6" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F2UA8823C%22%2C%22annotationKey%22%3A%22C9FU386W%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B45.646408839779%2C356.95%2C303.1823204419889%2C557.006%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%226%22%7D%7D" width="429" height="333" src="attachments/CFCJ7BJ6.png" ztype="zimage">](attachments/CFCJ7BJ6.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/Z5MLKUYC">Miyato 等, 2018, p. 6</a></span>)</span>

VAT是一种通过似然梯度和用上图计算的梯度 $\nabla_{\theta}\mathcal{R}_{vadv}$ 的加权和对模型进行更新的算法。

#### 虚拟对抗训练 vs 随机扰动训练

用于VAT的正则化函数通常可以写成：

![\<img alt="" data-attachment-key="TQBUWQEQ" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F2UA8823C%22%2C%22annotationKey%22%3A%22G22YWWI9%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B45.646%2C233.7613397250518%2C294.729%2C281.0983562996375%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%226%22%7D%7D" width="415" height="79" src="attachments/TQBUWQEQ.png" ztype="zimage">](attachments/TQBUWQEQ.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/Z5MLKUYC">Miyato 等, 2018, p. 6</a></span>)</span>

其中 $r_K$ 是在样本球 $U(r|\epsilon)$ 上应用 $K$ 次幂迭代获得的，该样本球是从均匀分布中获取的，半径为 $\epsilon$ 。实践中，对于上式的计算，使用有关随机扰动 $r_K$ 的经验期望。

为了执行VAT，使用 $K\geq 1$ 的这个正则化项。同时，将带有 $\mathcal{R}^{(0)}$ 的训练称为随机扰动训练(RPT)。RPT是一个VAT的降级版本，不执行幂迭代。根据定义，RPT只在每个输入数据点周围各向同性地平滑函数。相比于RPT，VAT在减少泛化误差方面具有更强的能力：

VAT的学习过程本质上比RPT更稳定；RPT的正则化函数对模型有本质影响。

## 结论及改进方向

### 实验

使用 $p(y|x,\theta)$ 来表示分类器的标签分布，其中 $\theta$ 代表NN的参数向量。关于激活函数，使用ReLU，使用Batch Normalization。对于差异 $D$ ，选择了 KL 散度：

![\<img alt="" data-attachment-key="5U2P7DKZ" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F2UA8823C%22%2C%22annotationKey%22%3A%22RDJSSHDG%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B55.79%2C668.022%2C269.37%2C699.58%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%227%22%7D%7D" width="356" height="53" src="attachments/5U2P7DKZ.png" ztype="zimage">](attachments/5U2P7DKZ.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/Z5MLKUYC">Miyato 等, 2018, p. 7</a></span>)</span>

对于分类问题， $Q$ 是所有可能标签的集合。

#### MNIST和CIFAR-10上的监督学习

![\<img alt="" data-attachment-key="9SYF858X" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F2UA8823C%22%2C%22annotationKey%22%3A%22DYQCX4HH%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B59.735%2C210.994%2C292.475%2C305.669%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%227%22%7D%7D" width="388" height="158" src="attachments/9SYF858X.png" ztype="zimage">](attachments/9SYF858X.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/Z5MLKUYC">Miyato 等, 2018, p. 7</a></span>)</span>

上图显示了 $\mathcal{R}\_{vadv}$ 的转变以及在没有使用VAT训练的基准NN(表示为'wo/VAT')和使用VAT训练的NN(表示为'w/VAT')的学习曲线。随着训练的进行，使用VAT训练的NN的 $\mathcal{R}\_{vadv}$ 超过了基准的 $\mathcal{R}\_{vadv}$ ，也就是说，在 $LDS$ 方面，使用VAT训练的模型比基准模型更加平稳。

![\<img alt="" data-attachment-key="QVWY7IHB" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F2UA8823C%22%2C%22annotationKey%22%3A%22UFPTKFEH%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B325.16%2C376.11%2C552.265%2C492.199%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%227%22%7D%7D" width="379" height="194" src="attachments/QVWY7IHB.png" ztype="zimage">](attachments/QVWY7IHB.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/Z5MLKUYC">Miyato 等, 2018, p. 7</a></span>)</span>

上图总结了正则化方法(VAT)和其他监督学习的正则化方法在MNIST的表现，VAT除了阶梯网络之外，比所有同期方法都表现得更好。

![\<img alt="" data-attachment-key="MZZQGYTF" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F2UA8823C%22%2C%22annotationKey%22%3A%22ZL8CTDT6%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B341.503%2C184.508%2C533.105%2C294.961%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%227%22%7D%7D" width="319" height="184" src="attachments/MZZQGYTF.png" ztype="zimage">](attachments/MZZQGYTF.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/Z5MLKUYC">Miyato 等, 2018, p. 7</a></span>)</span>

上图总结了使用CNN实施的有监督学习方法在CIFAR10上的测试性能。将VAT的表现与像ResNet 和DenseNet等先进的架构进行了比较，以确认算法的基线模型足够“普通”，以便可以正确地将算法的有效性归因于其本身的规则化方式，而不是在实验中使用的网络结构。

#### MNIST、SVHN 和 CIFAR-10 上的半监督学习

![\<img alt="" data-attachment-key="8GRRHKUD" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F2UA8823C%22%2C%22annotationKey%22%3A%22X79C6YAH%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B71.569%2C361.459%2C273.878%2C495.58%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%228%22%7D%7D" width="337" height="223" src="attachments/8GRRHKUD.png" ztype="zimage">](attachments/8GRRHKUD.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%228%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/Z5MLKUYC">Miyato 等, 2018, p. 8</a></span>)</span>

上图总结了打乱排列不变MNIST任务的结果，列出的所有方法都属于半监督学习方法家族。对于MNIST数据集，VAT表现出色，除了基于生成模型的方法以外，它超越了所有当代方法，这些方法是最先进的方法。

在 SVHN 和 CIFAR-10 实验中，采用 $p(y|x,\theta)$ 条件熵作为额外的成本：

![\<img alt="" data-attachment-key="NE49NRMU" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F2UA8823C%22%2C%22annotationKey%22%3A%22HG6E77GI%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B55.227%2C187.326%2C289.657%2C231.845%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%228%22%7D%7D" width="391" height="74" src="attachments/NE49NRMU.png" ztype="zimage">](attachments/NE49NRMU.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%228%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/Z5MLKUYC">Miyato 等, 2018, p. 8</a></span>)</span>

条件熵最小化对模型 $p(y|x,\theta)$ 在每个数据点的预测产生夸大的影响。对于半监督图像分类任务，这种额外的成本尤其有帮助。“VAT+EntMin”表示使用 $\mathcal{R}\_{vadv}+ \mathcal{R}\_{cent}$ 进行训练。

![\<img alt="" data-attachment-key="QMVNI9XH" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F2UA8823C%22%2C%22annotationKey%22%3A%22CPJ75VAK%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B316.144%2C420.63%2C558.464%2C590.254%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%228%22%7D%7D" width="404" height="283" src="attachments/QMVNI9XH.png" ztype="zimage">](attachments/QMVNI9XH.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%228%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/Z5MLKUYC">Miyato 等, 2018, p. 8</a></span>)</span>

上图总结了对SVHN和CIFAR-10的半监督学习任务的结果。方法在VAT的帮助下实现了14.82(%)的测试错误率，这优于CIFAR-10半监督学习的最先进方法。

![\<img alt="" data-attachment-key="ANGKAGXV" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F2UA8823C%22%2C%22annotationKey%22%3A%22DZN3QWB3%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%229%22%2C%22position%22%3A%7B%22pageIndex%22%3A8%2C%22rects%22%3A%5B%5B55.227%2C515.304%2C290.221%2C620.122%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%229%22%7D%7D" width="392" height="175" src="attachments/ANGKAGXV.png" ztype="zimage">](attachments/ANGKAGXV.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%229%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/Z5MLKUYC">Miyato 等, 2018, p. 9</a></span>)</span>

上图展示了使用现代图像数据增强（平移和水平翻转）实现的VAT和当代半监督学习方法的性能。除了VAT之外的所有方法，在对无标签训练样本进行处理时都假设图像的标签不会因变形而改变。TAX的影响与数据增强方法的影响并不重叠，因此它们可以结合使用以提高性能。

#### 扰动大小和正则化系数的影响

在之前的所有实验中，VAT在固定 $\alpha=1$ 的情况下取得了竞争性的结果。对于较小的 $\epsilon$ ，VAT 正则化的强度与两个超参数 $\alpha$ 和 $\epsilon^2$ 的乘积成正比，即在较小的 $\epsilon$ 区域，只搜索 $\epsilon$ 或 $\alpha$ 中的一个超参数即可。然而，当考虑一个相对较大的 $\epsilon$ 值时，超参数 $\alpha$ 和 $\epsilon$ 就不能放到一起了。在这种情况下，寻找最佳的超参数组合(优先考虑对 $\epsilon$ 的参数搜索，而不是对 $\alpha$ 的搜索)，以达到最佳性能是必要的。

![\<img alt="" data-attachment-key="9CK3E7PX" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F2UA8823C%22%2C%22annotationKey%22%3A%22RISK2ELK%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%229%22%2C%22position%22%3A%7B%22pageIndex%22%3A8%2C%22rects%22%3A%5B%5B316.707182320442%2C631.2795511319492%2C559.0276243093922%2C745.1138052755956%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%229%22%7D%7D" width="404" height="190" src="attachments/9CK3E7PX.png" ztype="zimage">](attachments/9CK3E7PX.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%229%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/Z5MLKUYC">Miyato 等, 2018, p. 9</a></span>)</span>

上图显示了 $\epsilon$ 和 $\alpha$ 对于在MNIST上进行监督学习的验证性能的影响。在图a中，展示了在 $\alpha=1$ 固定值下对 进行的影响，而在图b中，展示了 $\epsilon$ 在 {1.0、2.0、3.0}范围内不同固定值下对 $\alpha$ 的影响。

#### 幂迭代次数K的效果

![\<img alt="" data-attachment-key="MHR6WLVT" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F2UA8823C%22%2C%22annotationKey%22%3A%22PJ2D7TZS%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%229%22%2C%22position%22%3A%7B%22pageIndex%22%3A8%2C%22rects%22%3A%5B%5B311.635%2C184.508%2C563.536%2C318.63%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%229%22%7D%7D" width="420" height="224" src="attachments/MHR6WLVT.png" ztype="zimage">](attachments/MHR6WLVT.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%229%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/Z5MLKUYC">Miyato 等, 2018, p. 9</a></span>)</span>

上图显示了使用不同K(幂迭代的次数)在MNIST上进行监督学习和在CIFAR-10上进行半监督学习训练的模型的 $\mathcal{R}\_{vadv}$ 值。可以观察到，在从 K = 0(随机扰动)到 K = 1(虚拟对抗性扰动)的转变过程中， $\mathcal{R}\_{vadv}$ 的值显著增加。还可以观察到，该值在K = 1时饱和，事实上一个幂迭代就能达到良好的性能表明。

![\<img alt="" data-attachment-key="37LDCBWT" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F2UA8823C%22%2C%22annotationKey%22%3A%22TXSTGLUN%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2210%22%2C%22position%22%3A%7B%22pageIndex%22%3A9%2C%22rects%22%3A%5B%5B97.492%2C631.392%2C249.083%2C710.851%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%2210%22%7D%7D" width="253" height="133" src="attachments/37LDCBWT.png" ztype="zimage">](attachments/37LDCBWT.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%2210%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/Z5MLKUYC">Miyato 等, 2018, p. 10</a></span>)</span>

上图显示了在带有不同K值的CIFAR10的半监督学习任务中的测试准确性。事实上，通过增加K的值，无法取得明显的性能改进。

#### 虚拟对抗样本的可视化

##### 使用不同扰动选择训练的模型产生的虚拟对抗样本

![\<img alt="" data-attachment-key="77WKJL27" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F2UA8823C%22%2C%22annotationKey%22%3A%22E2JPEYL7%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2211%22%2C%22position%22%3A%7B%22pageIndex%22%3A10%2C%22rects%22%3A%5B%5B95.3125%2C310.7499999999999%2C509.68750000000006%2C745.1249999999999%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%2211%22%7D%7D" width="691" height="724" src="attachments/77WKJL27.png" ztype="zimage">](attachments/77WKJL27.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%2211%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/Z5MLKUYC">Miyato 等, 2018, p. 11</a></span>)</span>

在上图中，对齐了(I)在与 $\epsilon$ 相关的SVHN和CIFAR-10上的 VAT 性能转换与(II)模型以相应的 $\epsilon$ 训练生成的实际虚拟对抗样本。

对于小的 $\epsilon$ \[在图中标记为(1)]，人眼很难区分虚拟对抗样本与清晰图像。

最佳验证性能的尺寸由(2)指定。特别是对于 CIFAR-10，带有 $\epsilon$ (2)的虚拟对抗样本即将完全损坏。

对于更大的值\[在图中标为(3)]，可以清晰地观察到过度正则化的效果。使用此范围内训练的模型生成的虚拟对抗性例子与干净图像非常不同，观察到使用这么大的范围实现的算法做出了在“不自然”的图像集上平滑输出分布的不必要的工作。

##### 训练后对抗虚拟样本的鲁棒性

在CIFAR-10上使用了有和没有VAT的CNN进行训练，并准备了一组由同一张图片生成的虚拟对抗性示例对，每个对抗性示例包含(1)由有VAT模型训练生成的虚拟对抗性示例(w/ VAT)和(2)由无VAT训练模型生成的虚拟对抗性示例(wo/ VAT)。研究了分类器（带VAT和不带VAT）在这些对抗性样本对上的误识率。

![\<img alt="" data-attachment-key="VC9E2RK5" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F2UA8823C%22%2C%22annotationKey%22%3A%22STEG3FHS%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2212%22%2C%22position%22%3A%7B%22pageIndex%22%3A11%2C%22rects%22%3A%5B%5B94.99999999999999%2C200.333%2C518.333%2C752.8330000000001%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%2212%22%7D%7D" width="706" height="921" src="attachments/VC9E2RK5.png" ztype="zimage">](attachments/VC9E2RK5.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FZ5MLKUYC%22%5D%2C%22locator%22%3A%2212%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/Z5MLKUYC">Miyato 等, 2018, p. 12</a></span>)</span>

上图显示两种模型(w/VAT和wo/VAT)在有不同幅度的虚拟对抗扰动干扰的图像上误识别率。

中间面板中的图(A)显示了使用VAT训练的模型生成的虚拟对抗性示例上所做的误识率。图(B)显示了未经过VAT训练的模型生成的虚拟对抗性样本的速率。

图 (A)和 (B)下面显示的示例图片是从一组被有VAT和没有VAT训练的模型中未受干扰时正确识别的图片中生成的对抗性示例。如预期的那样，两种模型的错误率都随着腐败程度的加剧而单调增加。

由图。具有 $\epsilon\sim10^{-1}\rightarrow10^0$ 的虚拟对抗示例是希望分类器不犯错误的示例。

可以清楚地观察到，有VAT训练模型在这个范围内误识别率明显低于没有使用VAT训练的模型。另外请注意，在底部面板中，针对这个范围，使用VAT训练的模型可以正确识别其自身生成的对抗性实例和未使用VAT训练的模型生成的对抗性实例。

没有进行VAT训练的模型将原始标签分配给过度扰动的图像的比例要高得多，这是完全不必要的，在实践中甚至可能是有害的，而受VAT训练的模型则不然，所以它更自然。

### 未来工作

从本质上讲，大多数基于生成模型的方法通过使模型对高 $p(x)$ 值区域中的扰动具有鲁棒性来提高泛化性能，区域很可能在未来接收到新的输入数据点。

原则上，这与本文的方法互补，本文的方法旨在在每个观察到的输入点上各向同性地平滑输出分布 $p(y|x)$ ，而不对输入分布 $p(x)$ 做出任何明确的假设。

这两个想法的结合是未来的工作。
