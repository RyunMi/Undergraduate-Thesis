# 通用域适应

## 引用方式

### GB/T 7714

<span style="background-color: rgb(255, 255, 255)">You K, Long M, Cao Z, et al. Universal domain adaptation[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 2720-2729.</span>

### Bibtex

    @inproceedings{you2019universal,
      title={Universal domain adaptation},
      author={You, Kaichao and Long, Mingsheng and Cao, Zhangjie and Wang, Jianmin and Jordan, Michael I},
      booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
      pages={2720--2729},
      year={2019}
    }

## 词汇

off-the-shelf 现成的

## 主要思想

域适应旨在存在领域差距的情况下迁移知识。现有的域适应方法依赖于关于源域和目标域的标签集之间关系的丰富先验知识，这极大地限制了它们在野外的应用。

本文介绍了不需要标签集先验知识的通用领域自适应，对于给定的源标签集和目标标签集，它们可能分别包含一个公共标签集和一个私有标签集，从而带来额外的类别差距。UDA要求模型(1)如果目标样本与公共标签集中的标签相关联，则对其进行正确分类；(2)否则将其标记为“未知”。

UDA模型应该在广泛的共性（公共标签集在完整标签集上的比例）下稳定工作，这样它就可以处理未知目标标签集的现实世界问题。

为了解决通用域适应问题，本文提出了通用适应网络UAN。它量化了样本级的可迁移性，以发现公共标签集和每个域专用的标签集，从而促进了自动发现的公共标签集的自适应，并成功识别“未知”样本。

全面评估表明，在新的UDA设置中，UAN优于现有技术的闭集、部分和开集域自适应方法。

## 主要内容

### 引论

域适应旨在最大限度地减少领域差距，并将在源域训练的模型成功转移到目标域。

现有的域适应方法通过学习领域不变特征表示、为目标域生成特征/样本或通过生成模型在领域之间转换样本来解决领域差距。他们认为标签集在不同领域是相同的。

最近的工作尝试放宽这一假设，提出了开集域适应和部分域适应。部分域适应要求源标签集合包含目标标签集合。开集域适应介绍两个域中都有"未知"类，并假设两个域之间的公共类在训练阶段是已知的。改进的开集域适应删除了源未知类的数据，使得源标签集是目标标签集的子集。

![\<img alt="" data-attachment-key="PN2X3GH2" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBDKF6CXY%22%2C%22annotationKey%22%3A%22A6L6HPEH%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222720%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B323.003126156692%2C328.1861988337848%2C534.867%2C539.768%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222720%22%7D%7D" width="353" height="353" src="attachments/PN2X3GH2.png" ztype="zimage">](attachments/PN2X3GH2.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222720%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/L62GQZCB">You 等, p. 2720</a></span>)</span>

在存在较大域间隙的情况下，源域和目标域之间的标签集关系是未知的。

本文提出的UDA中，给定一个带标签的源域，对于任何一个相关的目标域，无论其标签集合与源域的标签集合有何不同，如果某标签类属于源标签集合中的任何一类，都需要对其样本进行正确分类，否则将其标记为"未知"。"通用"一词表示UDA对标签集合不施加任何先验知识。

### 相关工作

#### 闭集域适应

针对闭集域适应的解决方法主要分为两类：特征适应和生成模型。

随着生成式对抗网络在图像合成方面取得重大进展，提出了利用生成模型匹配特征分布的方法。学习一个域分类器来区分源域和目标域的特征，并迫使特征提取器在对抗学习范式中混淆域分类器。

生成模型的方法可以合成标记目标样本作为数据增强，并且在像素和特征级别上匹配域。随着Cycle-Consistent GAN在图像翻译方面取得了令人印象深刻的成果，最近研究了基于CycleGAN的域适应方法，这些方法通常使用CycleGAN将源图像转换为类似目标图像的图像，然后再用转换后的图像和源图像分别对每个域的分类器进行训练。

#### 部分集域适应

Cao等人利用多个域判别器以及类别级别和实例级别加权机制，实现了每个类别的对抗分布匹配。Zhang等人构建了一个辅助域判别器，以量化源域样本与目标域相似的概率。Cao等人通过仅使用一个对抗网络，并联合在源分类器上应用类别级别加权，进一步改进了PDA。

#### 开集域适应

方法通过在已知共同类别的情况下放弃“未知”类别来解决领域差距问题。

### 通用域适应

#### 问题设置

训练时，源域 $\mathcal{D}_S=\{(x^s_i,y^s_i)\}$ 包含 $n_s$ 个标签实例，目标域 $\mathcal{D}_t=\{(x^t_i)\}$ 包含 $n_t$ 个标签实例。

源数据从分布 $p$ 采样，而目标数据从分布 $q$ 采样。使用 $\mathcal{C}\_s$ 表示源域的标签集， $\mathcal{C}\_t$ 表示目标域的标签集。 $\mathcal{C}=\mathcal{C}\_s\cap\mathcal{C}\_t$ 是两域共享的公共标签集， $\overline{C_s}=C_s\setminus C$ 和 $\overline{C_t}=C_t\setminus C$ 分别表示源域和目标域专用的标签集。 $p_{\mathcal{C}\_s}$ 和 $p_{\mathcal{C}}$ 分别用于表示具有 $\mathcal{C}\_s$ 和 $\mathcal{C}$ 中的标签的源域数据的分布，以及 $q_{\mathcal{C}\_t}$ 和 $q_{\mathcal{C}}$ 分别用于具有 $\mathcal{C}\_t$ 和 $\mathcal{C}$ 中的标签的目标域数据分布。目标数据是完全未标记的，目标标签集（在训练时无法使用）仅用于定义UDA问题。

将两个域之间的共性(commonness)定义为两个标签集的Jaccard距离： $\xi=\frac{\mathcal{C}\_s\cap\mathcal{C}\_t}{\mathcal{C}\_s\cup\mathcal{C}\_t}$ 。闭集域适应： $\xi=1$ ，UDA的任务是设计一个不知道 $\xi$ 但在 $\xi$ 的宽谱范围内工作良好的模型。其必须能够区分来自 $\mathcal{C}$ 和来自 $\mathcal{C}\_t$ 的目标数据，并学习分类模型 $f$ ，以最小化公共标签集中的目标风险，即 $\mathbb{E}\_{(x,y)\sim q_{\mathcal{C}}}[f(x)\neq y]$ 。

#### 技术挑战

需要从 $C$ 中自动识别源数据和目标数据，以便在自动发现的公共标签集中进行特征对齐。尽管存在类别差距，但UDA设置中即在公共标签集中的源数据和目标数据之间仍然存在域差距： $p\neq q,p_{\mathcal{C}}\neq q_{\mathcal{C}}$ 。应用域适应来对齐公共标签集 $C$ 中源数据和目标数据的分布。UDA面临的另一个挑战是检测“未知”类，在实践中，通常使用置信阈值，将分类置信度低的样本标记为“未知”。

#### 通用适应网络

![\<img alt="" data-attachment-key="9VKSDLYV" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBDKF6CXY%22%2C%22annotationKey%22%3A%227JCPLA66%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222723%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B64.01508567831115%2C533.012%2C530.362%2C718.245%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222723%22%7D%7D" width="777" height="309" src="attachments/9VKSDLYV.png" ztype="zimage">](attachments/9VKSDLYV.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222723%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/L62GQZCB">You 等, p. 2723</a></span>)</span>

特征提取器 $F$ ：输入来自任一域的图片 $x$ ，经过卷积层，输出图片特征 $z=F(x)$ 。

标签分类器 $G$ ：输入图片特征 $z$ ，经过全连接层，输出分类信息 $\hat{y}=G(z)$ ( $x$ 在源类 $\mathcal{C}_s$ 上的概率)。

损失函数(1)：

![\<img alt="" data-attachment-key="WSS3ZE28" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBDKF6CXY%22%2C%22annotationKey%22%3A%22CXMXUYWD%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222723%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B360.894%2C401.26599999999996%2C492.64%2C416.4673413063476%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222723%22%7D%7D" width="220" height="25" src="attachments/WSS3ZE28.png" ztype="zimage">](attachments/WSS3ZE28.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222723%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/L62GQZCB">You 等, p. 2723</a></span>)</span>

用源域的图片 $x$ 对 $F,G$ 做监督训练。

非对抗性域判别器 $D'$ ：量化样本 $x$ 与源域的相似性。

样本属于源域 $\Rightarrow\hat{d}'\rightarrow1$ ，样本属于目标域 $\Rightarrow\hat{d}'\rightarrow0$ (domain similarity-- $\hat{d}'=D'(z)$ )。通过损失函数(2)实现：

![\<img alt="" data-attachment-key="SXT8NC4U" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBDKF6CXY%22%2C%22annotationKey%22%3A%2263UZ99L7%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222723%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B356.953%2C364.107%2C496.581%2C393.947%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222723%22%7D%7D" width="233" height="50" src="attachments/SXT8NC4U.png" ztype="zimage">](attachments/SXT8NC4U.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222723%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/L62GQZCB">You 等, p. 2723</a></span>)</span>

对抗性域判别器 $D$ ：旨在对抗性地匹配落在公共标签集 $C$ 中的源数据和目标数据的特征分布。

损失函数(3)：

![\<img alt="" data-attachment-key="6RMMG4MU" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBDKF6CXY%22%2C%22annotationKey%22%3A%228DKSP88I%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222723%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B349.634%2C331.452%2C505.59%2C359.603%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222723%22%7D%7D" width="260" height="47" src="attachments/6RMMG4MU.png" ztype="zimage">](attachments/6RMMG4MU.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222723%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/L62GQZCB">You 等, p. 2723</a></span>)</span>

$w^s(x)$ ：源样本属于公共标签集的概率； $w^t(x)$ ：目标样本属于公共标签集的概率。损失函数(3)表示：利用已建立的加权 $w^s(x)$ 和 $w^t(x)$ ，对抗性域判别器 $D$ 被限制为区分公共标签集 $C$ 中的源数据和目标数据。

特征提取器 $F$ 试图混淆 $D$ ，从而在公共标签集 $C$ 中产生域不变特征。基于这些特征训练的标签分类器 $G$ 可以安全地应用于目标域。

##### 训练阶段

![\<img alt="" data-attachment-key="9VU9FJFN" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBDKF6CXY%22%2C%22annotationKey%22%3A%22XBV2ZX5K%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222723%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B380.037%2C119.194%2C474.061%2C161.42%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222723%22%7D%7D" width="157" height="71" src="attachments/9VU9FJFN.png" ztype="zimage">](attachments/9VU9FJFN.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222723%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/L62GQZCB">You 等, p. 2723</a></span>)</span>

对于第一个目标函数，

首先固定 $D$ ，最小化的过程中， $E_G$ 的最小化是降低源域 $\mathcal{C}_s$ 上的训练损失， $-\lambda E_D$ 的最小化是找到 $F$ 使得 $\mathcal{C}$ 上源域数据的输出 $D(F(x))$  尽可能小， $\mathcal{C}$ 上目标域数据的输出 $D(F(x))$ 尽可能大。\[混淆能力]

之后固定 $F,G$ ，最大化的过程中， $-\lambda E_D$ 的最大化是找到 $D$ 使得 $\mathcal{C}$ 上源域数据的输出 $D(F(x))$  尽可能大， $\mathcal{C}$ 上目标域数据的输出 $D(F(x))$ 尽可能小。\[判别能力]

对于第二个目标函数，最小化等于最大化 $-E_{D'}$ ，即找到 $D'$ 使得源域数据的输出 $D'(F(x))$  尽可能大，更倾向判别样本属于源域，反之使得目标域数据的输出 $D'(F(x))$  尽可能小，更倾向判别样本属于目标域。

利用DANN提出的成熟的梯度反转层来反转 $F$ 和 $D$ 之间的梯度，以优化端到端训练框架中的所有模块。

##### 测试阶段

给定每个输入目标样本 $x$ 、其在源标签集 $\mathcal{C}_s$ 上的分类预测 $\hat{y}(x)$ 以及域预测 $\hat{d}'(x)$ ，计算 $w^t(x)$ 。在确定的阈值 $w_0$ 下，通过阈值化关于 $w_0$ 的 $\hat{y}(x)$ ，可以预测类别 $y(x)$ ：

![\<img alt="" data-attachment-key="Y6ZQ38BV" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBDKF6CXY%22%2C%22annotationKey%22%3A%22CP8IB22R%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222724%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B96.276%2C583.684%2C238.156%2C615.776%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222724%22%7D%7D" width="236" height="53" src="attachments/Y6ZQ38BV.png" ztype="zimage">](attachments/Y6ZQ38BV.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222724%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/L62GQZCB">You 等, p. 2724</a></span>)</span>

要么拒绝目标样本 $x$ 为"未知"类，要么将其归为源类之一。

#### 可迁移性准则

如何通过样本级可迁移性准则计算权重 $w^s(x)$ 和 $w^t(x)$ 。

一个完善的样本级可迁移性准则应满足：

![\<img alt="" data-attachment-key="3D67V5TR" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBDKF6CXY%22%2C%22annotationKey%22%3A%2253KTFFI9%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222724%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B105.284%2C370.863%2C230.837%2C406.333%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222724%22%7D%7D" width="209" height="59" src="attachments/3D67V5TR.png" ztype="zimage">](attachments/3D67V5TR.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222724%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/L62GQZCB">You 等, p. 2724</a></span>)</span>

列出现有关于每个输入 $x$ 的内容：$\hat{y},\hat{d},\hat{d}$  。由于 $D$ 参与对抗性训练并被愚弄，其输出的 $\hat{d}$ 判别力不够。因此对 $\hat{y}$ 和 $\hat{d}'$ 的性质进行分析。

##### Domain Similarity

由前文，对于源样本，较小的 $\hat{d}’$ 意味着它与目标域更相似；对于目标样本，更大的 $\hat{d}’$ 意味着它与源域更相似：

![\<img alt="" data-attachment-key="ZP5C3IYU" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBDKF6CXY%22%2C%22annotationKey%22%3A%22TJJME4LX%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222724%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B65.31000000000002%2C195.76399999999992%2C255.61%2C212.7352941176471%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222724%22%7D%7D" width="317" height="28" src="attachments/ZP5C3IYU.png" ztype="zimage">](attachments/ZP5C3IYU.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222724%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/L62GQZCB">You 等, p. 2724</a></span>)</span>

第二个大于号自然成立。由于 $p_\mathcal{C}$ 和 $q_{\mathcal{C}}$ 共享相同的标签集，与 $q_{\overline{\mathcal{C}}\_t}$ 相比， $p_\mathcal{C}$ 更接近 $q_{\mathcal{C}}$ ，因此可以合理地假设第一个大于号成立，同样的观察结果也适用于第三个大于号。

##### Prediction Uncertainty

预测 $\hat{y}$ 包含关于输入的判别信息，但它仅在由标记数据保证的源域中是可靠的。为了利用未标记的数据，熵最小化被用作半监督学习和域适应中的标准，以强制未标记数据中的决策边界通过低密度区域。

原则上，熵量化了预测的不确定性，熵越小意味着预测越有信心。假设：

$\mathbb{E}\_{\mathbf{x} \sim q_{\overline{\mathcal{C}}\_t}} H(\hat{\mathbf{y}})>\mathbb{E}\_{\mathbf{x} \sim q_{\mathcal{C}}} H(\hat{\mathbf{y}})>\mathbb{E}\_{\mathbf{x} \sim p_{\mathcal{C}}} H(\hat{\mathbf{y}})>\mathbb{E}\_{\mathbf{x} \sim p_{\overline{\mathcal{C}}\_{\mathcal{S}}}} H(\hat{\mathbf{y}})$

由于源域是标记的，而目标域是未标记的，因此预测对于源样本是确定的，而对于目标样本是不确定的，那么第二个大于号自然成立。来自 $q_{\mathcal{C}}$ 和 $p_\mathcal{C}$ 的相似样本可以相互吸引，因此来自 $p_\mathcal{C}$ 的样本的熵变得更大，因其受到来自 $q_{\mathcal{C}}$ 的高熵样本的影响。而由于 $\overline{\mathcal{C}}\_s$ 与 $\mathcal{C}\_t$ 没有交集，来自 $p_{\overline{\mathcal{C}}\_s}$ 的样本不受目标数据的影响，并保持最高的确定性，所以第三个大于号成立。类似地， $\overline{\mathcal{C}}\_t$ 与 $\mathcal{C}\_s$ 没有交集（来自 $q_{\overline{\mathcal{C}}\_t}$ 的数据不属于 $\mathcal{C}\_s$ 中的任何类），所以第一个大于号成立。

通过以上分析，源数据点和目标数据点的样本级可迁移性标准可以分别定义为：

![\<img alt="" data-attachment-key="X5G65R9F" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBDKF6CXY%22%2C%22annotationKey%22%3A%22NLKN3LJF%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222724%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B373.281%2C378.745%2C481.37999999999994%2C441.5779243195506%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222724%22%7D%7D" width="180" height="105" src="attachments/X5G65R9F.png" ztype="zimage">](attachments/X5G65R9F.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222724%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/L62GQZCB">You 等, p. 2724</a></span>)</span>

熵是通过其最大值( $log|\mathcal{C}_s|$ )进行归一化的，因此它被限制在\[0,1]，并且与域相似性度量的值 $\hat{d}'$ 可比。此外在训练时，权重被归一化为区间\[0,1]。

所提出的通用自适应网络UAN利用样本级可迁移性标准来区分 $\mathcal{C}, \overline{\mathcal{C}}_s$ 中的源数据和 $\mathcal{C}, \overline{\mathcal{C}}_t$ 中的目标数据，因此减少了类别差距。通过对齐共享标签集 $\mathcal{C}$ 中域之间的特征，也减少了域间隙。

## 结论及改进方向

### 实验

#### 数据集

Office-31事实上适用于视觉域适应，在3个视觉不同的领域(A,D,W)中有31个类别。本文使用Office-31和Caltech-256共享的10个类作为公共标签集 $\mathcal{C}$ ，然后按字母顺序，后10个类用作 $\overline{\mathcal{C}}_s$ ，剩余的11个类用作 $\overline{\mathcal{C}}_t$ ， $\xi=0.32$ 。

Office Home是一个更大的数据集，在4个不同的领域中有65个对象类别：艺术图像(Ar)、剪贴画图像(Cl)、产品图像(Pr)和真实世界图像(Rw)。按照字母顺序，使用前10个类作为 $\mathcal{C}$ ，接下来的5个类作为 $\overline{\mathcal{C}}_s$ ，其余的作为 $\overline{\mathcal{C}}_t$ ， $\xi=0.15$ 。

VisDA2017数据集专注于一个特殊的域适应设置(模拟到真实)。源域由游戏引擎生成的图像组成，目标域由真实世界的图像组成。这个数据集中有12个类，使用前6个类作为 $\mathcal{C}$ ，接下来的3个类作为 $\overline{\mathcal{C}}_s$ ，其余的作为 $\overline{\mathcal{C}}_t$ ， $\xi=0.50$ 。

ImageNet-Caltech由ImageNet-1K(1000个类)和Caltech-256(256个类)组成。使用两个域共享的84个公共类作为公共标签集 $\mathcal{C}$ ，并分别使用它们的私有类作为私有标签集。该数据集自然属于通用域适应范式。本文形成了两个通用的域适应任务： $I\rightarrow C,C\rightarrow I$ ， $\xi=0.07$ 。

#### 评估协议

采用VisDA2018 Open Set Classification Challenge中的评估协议，其中目标私有标签集中的所有数据都被视为一个统一的“未知”类，所有 $|\mathcal{C}|+1$ 类的每个类的平均准确度是最终结果。

#### 分类结果

![\<img alt="" data-attachment-key="XHWQAUST" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBDKF6CXY%22%2C%22annotationKey%22%3A%22JN8W27IR%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222725%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B50.672%2C418.156%2C545.564%2C724.438%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222725%22%7D%7D" width="825" height="511" src="attachments/XHWQAUST.png" ztype="zimage">](attachments/XHWQAUST.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222725%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/L62GQZCB">You 等, p. 2725</a></span>)</span>

现有方法在UDA设置下容易出现负迁移，这意味着它们的性能比只在源数据上训练而不进行任何调整的模型差。

![\<img alt="" data-attachment-key="UBGMTQWG" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBDKF6CXY%22%2C%22annotationKey%22%3A%228Y2UNBW5%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222727%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B81.075%2C518.374%2C282.635%2C718.245%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222727%22%7D%7D" width="336" height="333" src="attachments/UBGMTQWG.png" ztype="zimage">](attachments/UBGMTQWG.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222727%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/L62GQZCB">You 等, p. 2727</a></span>)</span>

由上图，与任务 $Ar\rightarrow CI$ 上的ResNet相比，每类精度的增益。可以发现，DANN、IWAN和OSBP在大多数类别中都存在负迁移，并且只能促进少数类别的适应，只有UAN促进所有任务的正向迁移。

由于UDA违反了以前开放集DA方法的假设，因此它们的准确性急剧下降也就不足为奇了。

#### 不同UDA设置的分析

![\<img alt="" data-attachment-key="VR34QCJ4" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBDKF6CXY%22%2C%22annotationKey%22%3A%22V83MBTQ5%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222726%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B45.041%2C578.616%2C401.8255732588078%2C725.001%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222726%22%7D%7D" width="595" height="244" src="attachments/VR34QCJ4.png" ztype="zimage">](attachments/VR34QCJ4.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222726%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/L62GQZCB">You 等, p. 2726</a></span>)</span>

#### 通用适应网络分析

##### 消融实验

(1) UAN w/o d : 在计算 $w^s(x)$ 和 $w^t(x)$ 的样本级可迁移性准则中，没有将域相似性整合进去的变体。

(2) UAN w/o y : 在计算 $w^s(x)$ 和 $w^t(x)$ 中未将不确定性标准整合到样本级可迁移性标准中的变体。

结果显示在前文表1的底部行。UAN的性能优于UAN w/o d和UAN w/o y，这表明在 $w^s(x)$ 和 $w^t(x)$ 的定义中，领域相似性分量和不确定性标准分量都是必要的。此外，UAN w/o d比UAN w/o y表现更好，这意味着将不确定性标准整合到样本水平的可迁移性标准中更为关键。

##### 假设理由

为证明可迁移性假设的有效性，绘制了 $w^s(x)$ 的不同组成部分和 $w^t(x)$ 的不同组成部分的估计概率密度函数。

![\<img alt="" data-attachment-key="VUEJ26IU" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBDKF6CXY%22%2C%22annotationKey%22%3A%225MZL9TPC%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222727%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B322.6089999999999%2C499.90653002053756%2C517.8071739947784%2C717.119%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222727%22%7D%7D" width="325" height="362" src="attachments/VUEJ26IU.png" ztype="zimage">](attachments/VUEJ26IU.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222727%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/L62GQZCB">You 等, p. 2727</a></span>)</span>

结果表明，所有的假设都是成功的，这解释了为什么UAN可以在各种UDA设置中表现良好。另一个观察结果是，不确定性标准和领域相似性本身可以用来区分公共标签集和私有标签集中的所有示例。通过将这两个成分结合起来，可获得更具区分性的可迁移性准则。

##### 阈值敏感性

在 $I\rightarrow C$ 任务中探索了UAN对于阈值 $w_0$ 的敏感性。如下图所示，尽管UAN的准确度关于 $w_0$ 有约2％的差别，但它在很大范围的 $w_0$ 中始终表现出色，远远优于其他方法。

![\<img alt="" data-attachment-key="DMLZFRU7" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBDKF6CXY%22%2C%22annotationKey%22%3A%22FINCTUHJ%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222726%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B390.56522367279126%2C579.742%2C537.682%2C727.816%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222726%22%7D%7D" width="245" height="247" src="attachments/DMLZFRU7.png" ztype="zimage">](attachments/DMLZFRU7.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FL62GQZCB%22%5D%2C%22locator%22%3A%222726%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/L62GQZCB">You 等, p. 2726</a></span>)</span>

### 结论

在实践中，如果想将模型推广到一个新的场景，所提出的UAN可以是一个很好的候选模型。如果UAN将大多数例子归类为“未知”，那么在这样一个新场景中的领域自适应很可能会失败，收集标签将是必不可少的。另一方面，如果UAN可以为大多数示例生成标签，则不需要为这样的场景收集标签，域适应将执行这项工作。也就是说，当遇到新的领域适应场景时，UAN可以作为一项试点研究。
