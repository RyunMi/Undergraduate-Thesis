# 迁移学习研究综述

## 引用方式

### GB/T 7714

<span style="background-color: rgb(255, 255, 255)">Pan S J, Yang Q. A survey on transfer learning[J]. IEEE Transactions on knowledge and data engineering, 2010, 22(10): 1345-1359.</span>

### BibTex

    @article{pan2010survey,
      title={A survey on transfer learning},
      author={Pan, Sinno Jialin and Yang, Qiang},
      journal={IEEE Transactions on knowledge and data engineering},
      volume={22},
      number={10},
      pages={1345--1359},
      year={2010},
      publisher={IEEE}
    }

## 主要思想

许多机器学习和数据挖掘算法中的一个主要假设是，训练数据和测试数据必须在相同的特征空间中，并且具有相同的分布。然而，在许多实际应用中，这种假设可能不成立。例如，有时在一个感兴趣的领域中有一个分类任务，但在另一个感兴趣的领域中只有足够的训练数据，其中后一个数据可能在不同的特征空间中或遵循不同的数据分布。在这种情况下，如果知识转移成功，将通过避免昂贵的数据标记（当分布发生变化时，大多数统计模型需要使用新收集的训练数据从头开始重建）工作，大大提高学习性能。近年来，迁移学习已经成为解决这一问题的一种新的学习框架。这篇综述的重点是分类和回顾目前迁移学习在分类、回归和聚类问题上的进展。在这篇综述中，讨论了迁移学习与其他相关机器学习技术之间的关系，如领域自适应、多任务学习和样本选择偏差，以及协变量移位。本文还探讨了迁移学习研究中一些潜在的未来问题。

## 主要内容

迁移学习相关例子：

Web文档分类问题：将给定的Web文档分类为几个预定义的类别（新建的网页相比预定义类别改变）；

室内WiFi定位问题：基于先前收集的WiFi数据检测用户的当前位置（时间、设备或其他动态因素改变）；

情感分类问题：自动将对某个产品的评论分类为正面和负面观点（产品改变）。

下图显示了传统学习技术和迁移学习技术的学习过程之间的差异：传统的机器学习技术试图从头开始学习每一项任务，而迁移学习技术则试图在前一项任务的高质量训练数据较少时将知识转移到目标任务。

![\<img alt="" data-attachment-key="YZ8NWHA9" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FY9Q75CLZ%22%2C%22annotationKey%22%3A%22LYLB4JAX%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B290.761%2C622.542%2C537.024%2C721.9%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%222%22%7D%7D" width="410" height="165" src="attachments/YZ8NWHA9.png" ztype="zimage">](attachments/YZ8NWHA9.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%222%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/CQHCXPPZ">Pan 和 Yang, 2010, p. 2</a></span>)</span>

### 符号与定义

领域 $\mathcal{D}$ :包含两个部分，一个是特征空间 $\mathcal{X}$ ，另一个是边际概率分布 $P(X)$ ，其中 $X=\{x_1,\dots,x_n\}\in \mathcal{X}$ 。通常，如果两个域不同，则它们可能具有不同的特征空间或不同的边际概率分布。

对于特定域 $\mathcal{D}=\{\mathcal{X},P(X)\}$ ，一个任务由两个成分组成：一个标签空间 $\mathcal{Y}$ 和目标预测函数 $f(\cdot)$ （由 $\mathcal{T}=\{\mathcal{Y},f(\cdot)\}$ 表示），它没有被观察到，但可以从训练数据中学习，训练数据由 $\{x_i,y_i\}$ 组成，其中 $x_i\in \mathcal{X},y_i \in \mathcal{Y}$ 。函数 $f(\cdot)$ 可用于预测新实例 $x$ 的相应标签 $f(x)$ ，从概率的角度来看， $f(\cdot)$ 可以写成 $P(y|x)$ 。

一个源域 $\mathcal{D}\\_S$ 一个目标域 $\mathcal{D}_T$ 。更具体地，将源域数据表示为 $D_S=\{(x_{S_1},y_{S_1}),\dots,(x_{S_{n_S}},y_{S_{n_S}})\}$ ，其中 $x_{S_i}\in\mathcal{X}_S$ 是数据实例， $y_{S_i}\in\mathcal{Y}_S$ 是相应的类标签。将目标域数据表示为 $D_T=\{(x_{T_1},y_{T_1}),\dots,(x_{T_{n_T}},y_{T_{n_T}})\}$ ，其中 $x_{T_i}\in\mathcal{X}_T$ 是数据实例， $y_{T_i}\in\mathcal{Y}_T$ 是相应的输出。在大多数情况下， $0\leq n_T\ll n_S$ 。

**迁移学习：**给定源域 $\mathcal{D}_S$ 和学习任务 $\mathcal{T}_S$ 、目标域 $\mathcal{D}_T$ 和学习任务 $\mathcal{T}_T$ ，迁移学习旨在利用 $\mathcal{D}_S$ 和 $\mathcal{T}_S$ 中的知识帮助改进 $\mathcal{D}_T$ 中的目标预测函数 $f_T(\cdot)$ 的学习，其中 $\mathcal{D}_S\neq\mathcal{D}_T$ 或 $\mathcal{T}_S\neq\mathcal{T}_T$ 。

在上述定义中，一个域指 $\mathcal{D}=\{\mathcal{X},P(X)\}$ ，因此条件 $\mathcal{D}_S\neq\mathcal{D}_T$ 要么意味着 $\mathcal{X}_S\neq\mathcal{X}_T$ 要么意味着 $P_S(X)\neq P_T(X)$ 。一个任务指 $\mathcal{T}=\{\mathcal{Y},P(Y|X)\}$ ，因此条件 $\mathcal{T}_S\neq\mathcal{T}_T$ 要么意味着 $\mathcal{Y}_S\neq\mathcal{Y}_T$ 要么意味着 $P(Y_S|X_S)\neq P(Y_T|X_T)$ 。

当两个域的特征空间之间存在某种显式或隐式关系时，称源域和目标域是相关的。

### 分类

![\<img alt="" data-attachment-key="7WV2LXVU" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FY9Q75CLZ%22%2C%22annotationKey%22%3A%22FESUGQ6E%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B46.814344489716916%2C635.952%2C519.8342648084421%2C710.92828685259%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%223%22%7D%7D" width="788" height="125" src="attachments/7WV2LXVU.png" ztype="zimage">](attachments/7WV2LXVU.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/CQHCXPPZ">Pan 和 Yang, 2010, p. 3</a></span>)</span>

![\<img alt="" data-attachment-key="XTVLLIQD" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FY9Q75CLZ%22%2C%22annotationKey%22%3A%22PY4KT67A%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B32.794424170991824%2C589.5035875176055%2C533.2446233741791%2C711.4159381152149%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%224%22%7D%7D" width="834" height="203" src="attachments/XTVLLIQD.png" ztype="zimage">](attachments/XTVLLIQD.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%224%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/CQHCXPPZ">Pan 和 Yang, 2010, p. 4</a></span>)</span>

![\<img alt="" data-attachment-key="QGQ6H37S" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FY9Q75CLZ%22%2C%22annotationKey%22%3A%223KJZR77F%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B91.31235245784441%2C483.9274937603103%2C478.9936273582429%2C721.656577425649%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%225%22%7D%7D" width="646" height="396" src="attachments/QGQ6H37S.png" ztype="zimage">](attachments/QGQ6H37S.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%225%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/CQHCXPPZ">Pan 和 Yang, 2010, p. 5</a></span>)</span>

![\<img alt="" data-attachment-key="DDDK9C6Q" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FY9Q75CLZ%22%2C%22annotationKey%22%3A%22Q6X2JD4Q%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B34.01354767696791%2C34.68046320577059%2C532.6350616211911%2C156.83699999999996%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%225%22%7D%7D" width="831" height="204" src="attachments/DDDK9C6Q.png" ztype="zimage">](attachments/DDDK9C6Q.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%225%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/CQHCXPPZ">Pan 和 Yang, 2010, p. 5</a></span>)</span>

![\<img alt="" data-attachment-key="PDPVHL7Z" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FY9Q75CLZ%22%2C%22annotationKey%22%3A%22BVBJBEEU%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B34.745%2C636.562%2C532.147%2C710.928%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%226%22%7D%7D" width="829" height="124" src="attachments/PDPVHL7Z.png" ztype="zimage">](attachments/PDPVHL7Z.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/CQHCXPPZ">Pan 和 Yang, 2010, p. 6</a></span>)</span>

### Inductive迁移学习

**定义：**给定源域 $\mathcal{D}_S$ 和学习任务 $\mathcal{T}_S$ 、目标域 $\mathcal{D}_T$ 和学习任务 $\mathcal{T}_T$ ，Inductive迁移学习旨在利用 $\mathcal{D}_S$ 和 $\mathcal{T}_S$ 中的知识帮助改进 $\mathcal{D}_T$ 中的目标预测函数 $f_T(\cdot)$ 的学习，其中 $\mathcal{T}_S\neq\mathcal{T}_T$ 。

大多数迁移学习方法都侧重于源域中有标记数据可用的情况。

#### 迁移实例知识

尽管源域数据不能直接重加权，但仍有某些数据部分可以与目标域中的一些标记数据一起重加权。

##### TrAdaBoost

假设源域和目标域数据使用完全相同的特征和标签集合，但数据在两个域中的分布不同。由于分布差异，一些源域数据可能对目标域的学习有用，但其中一些可能无用甚至有害。

尝试对源域数据进行迭代重新加权，以减少源域“坏”数据的影响，同时鼓励“好”数据为目标域做出更多贡献。

对于每一轮迭代，根据加权的源数据和目标数据训练基础分类器。仅根据目标数据计算误差。

回顾AdaBoost：采用错题集形式，每次迭代训练出分类器后，分类器预测错误的样本权重升高，预测正确的样本权重降低。对于TrAdaBoost，目标域中标签数据稀少，需要与源域数据一起训练分类器，在每轮权重调整时，目标域依然采用错题集形式，但TrAdaboost认为源域中分错的样本，与目标域更不相似，故降低权重。

#### 迁移特征表示的知识

##### 监督特征构造

基本思想是学习跨相关任务共享的低维表示。此外，学习的新表示也可以减少每个任务的分类或回归模型误差。

一种稀疏特征学习方法：共享特征可通过求解如下优化问题（同时估计模型的低维表示 $U^TX_T,U^TX_S$ 和参数 $A$ ）：

![\<img alt="" data-attachment-key="B8UZECI2" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FY9Q75CLZ%22%2C%22annotationKey%22%3A%22VEK5DFLT%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B313.315%2C540.251%2C521.175%2C588.7721152742546%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%226%22%7D%7D" width="346" height="81" src="attachments/B8UZECI2.png" ztype="zimage">](attachments/B8UZECI2.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/CQHCXPPZ">Pan 和 Yang, 2010, p. 6</a></span>)</span>

其中， $S$ 和 $T$ 分别表示源域和目标域中的任务。$A=[a_S,a_T]\in R^{d\times 2}$ 是一个参数矩阵。 $U$ 是用于将原始高维数据映射到低维表示的 $d\times d$ 正交矩阵（映射函数）。 $A$ 的 $(r,p)$ 范数定义为 $||A||_{r,p}\triangleq(\sum_{i=1}^d||a^i||_r^p)^{\frac{1}{p}}$ 。

##### 无监督特征构造

一种应用稀疏编码方法，学习用于迁移学习的更高级别的特征：

第一步，通过解决如下优化问题，在源域数据上学习到高维基向量 $b=\{b_1,b_2,\dots,b_s\}$ （其中， $a_{S_i}^j$ 是输入 $x_{S_i}$ 的基 $b_j$ 的新表示， $\beta$ 是平衡特征构造项和正则化项的系数）：

![\<img alt="" data-attachment-key="P8JMJQVG" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FY9Q75CLZ%22%2C%22annotationKey%22%3A%22Y79FW8PN%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B333.0162162162162%2C143.0589999999999%2C499.5243243243243%2C193.78399999999996%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%226%22%7D%7D" width="278" height="85" src="attachments/P8JMJQVG.png" ztype="zimage">](attachments/P8JMJQVG.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/CQHCXPPZ">Pan 和 Yang, 2010, p. 6</a></span>)</span>

第二步，对目标域数据应用如下优化算法，以基于基向量 $b$ 学习高维特征：

![\<img alt="" data-attachment-key="Z6456IAX" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FY9Q75CLZ%22%2C%22annotationKey%22%3A%22TNS7BI2H%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B323.092%2C30.584%2C497.87%2C66.422%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%226%22%7D%7D" width="291" height="60" src="attachments/Z6456IAX.png" ztype="zimage">](attachments/Z6456IAX.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/CQHCXPPZ">Pan 和 Yang, 2010, p. 6</a></span>)</span>

最后，可将判别算法应用于具有相应标签的 $\{a_{T_i}^*\}$ ，以训练用于目标域的分类或回归模型。

缺点：在源域上学习的所谓的高维基向量可能不适合在目标域中使用。

#### 迁移参数知识

假设相关任务的各个模型应该共享一些参数或超参数的先验分布。

在迁移学习中，不同域在损失函数中的权重可能不同。直观地说，可以为目标域的损失函数分配更大的权重，以确保在目标域中获得更好的性能。

一种多任务学习方法是在正则化框架下转移SVM的参数。方法假设每个任务的SVM中的参数 $w$ 可以被分成两个项。一个是任务通用项，另一个是任务特定项。

在Inductive迁移学习中 $w_S=w_0+v_S,w_T=w_0+v_T$ ，其中 $w_S$ 和 $w_T$ 分别是源任务和目标学习任务的SVM的参数。 $w_0$ 是公共参数，而 $v_S$ 和 $v_T$ 分别是源任务和目标任务的特定参数。通过假设 $f_t=w_t\cdot x$ 是任务 $t$ 的超平面，SVM对多任务学习情况的扩展可以写成如下：

![\<img alt="" data-attachment-key="BDAWABFT" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FY9Q75CLZ%22%2C%22annotationKey%22%3A%22X5Q2VDKQ%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B303.24299999999994%2C646.995%2C514.411%2C730.5794628246409%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%227%22%7D%7D" width="352" height="139" src="attachments/BDAWABFT.png" ztype="zimage">](attachments/BDAWABFT.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/CQHCXPPZ">Pan 和 Yang, 2010, p. 7</a></span>)</span>

通过解决上述优化问题，可以同时学习参数 $w_0,v_S,v_T$ 。

#### 迁移关系知识

处理关系域中的迁移学习问题，其中数据是非独立同分布的，可以由多种关系表示，例如网络数据和社交网络数据。该方法不同于传统假设：从每个域提取的数据是独立的，并且服从相同分布。它试图将数据之间的关系从源域传输到目标域。

一种通过马尔可夫逻辑网络（MLN）跨关系域传递关系知识的算法TAMAR的动机是，如果两个域彼此相关，则可能存在将实体及其关系从源域连接到目标域的映射。

例如，教授可以被视为在学术领域中扮演与工业管理领域经理类似的角色。此外，教授与学生之间的关系类似于经理与员工之间的关系。因此，可能存在从教授到经理的映射，以及从教授-学生关系到经理-员工关系的映射。

在这种情况下，TAMAR尝试使用为源域学习的MLN来帮助学习目标域的MLN。

### Transductive迁移学习

**定义：**给定源域 $\mathcal{D}_S$ 和学习任务 $\mathcal{T}_S$ 、目标域 $\mathcal{D}_T$ 和学习任务 $\mathcal{T}_T$ ，Transductive迁移学习旨在利用 $\mathcal{D}_S$ 和 $\mathcal{T}_S$ 中的知识帮助改进 $\mathcal{D}_T$ 中的目标预测函数 $f_T(\cdot)$ 的学习，其中 $\mathcal{D}_S\neq\mathcal{D}_T,\mathcal{T}_S=\mathcal{T}_T$ 。此外，一些未标记的目标域数据必须在训练时可用。

**域自适应：**源数据和目标数据的特征空间相同，边际概率分布不同。

可以通过一些未标记的目标域数据来调整在源域中学习的预测函数以用于目标域。

#### 迁移实例知识

大多数Transductive迁移学习环境的实例迁移方法都是由重要性抽样启发的。

首先回顾经验风险最小化（ERM）问题：通常希望通过最小化预期风险来学习模型的最优参数 $\theta^*$ （ $l(x,y,\theta)$ 是关于 $\theta$ 的损失函数）：

![\<img alt="" data-attachment-key="C24YRIQY" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FY9Q75CLZ%22%2C%22annotationKey%22%3A%22L8I9YLRD%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B346.032%2C575.935%2C475.452%2C600.613%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%228%22%7D%7D" width="216" height="41" src="attachments/C24YRIQY.png" ztype="zimage">](attachments/C24YRIQY.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%228%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/CQHCXPPZ">Pan 和 Yang, 2010, p. 8</a></span>)</span>

由于很难估计概率分布P，选择最小化ERM（ $n$ 是训练数据数量）：

![\<img alt="" data-attachment-key="B75UPMXX" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FY9Q75CLZ%22%2C%22annotationKey%22%3A%2265G34F7U%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B350.9680000000001%2C487.097%2C472.71000000000004%2C517.1483887703189%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%228%22%7D%7D" width="203" height="50" src="attachments/B75UPMXX.png" ztype="zimage">](attachments/B75UPMXX.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%228%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/CQHCXPPZ">Pan 和 Yang, 2010, p. 8</a></span>)</span>

在Transductive迁移学习环境中，希望通过最小化期望风险来学习目标域的最优模型：

![\<img alt="" data-attachment-key="HWVCPBNA" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FY9Q75CLZ%22%2C%22annotationKey%22%3A%22WIGDKAC4%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B336.71%2C403.194%2C485.871%2C433.355%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%228%22%7D%7D" width="249" height="50" src="attachments/HWVCPBNA.png" ztype="zimage">](attachments/HWVCPBNA.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%228%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/CQHCXPPZ">Pan 和 Yang, 2010, p. 8</a></span>)</span>

然而，由于在训练数据中没有观察到目标域中的标记数据，因此必须从源域数据中学习模型。如果 $P(D_S)=P(D_T)$ ，那么可以通过解决以下优化问题来简单地学习模型，以便在目标领域中使用:

![\<img alt="" data-attachment-key="GL9YHUWL" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FY9Q75CLZ%22%2C%22annotationKey%22%3A%22IWVEA9EJ%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B340.548%2C306.129%2C485.871%2C335.194%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%228%22%7D%7D" width="242" height="48" src="attachments/GL9YHUWL.png" ztype="zimage">](attachments/GL9YHUWL.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%228%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/CQHCXPPZ">Pan 和 Yang, 2010, p. 8</a></span>)</span>

否则，当 $P(D_S)\neq P(D_T)$ ，需要修改上述优化问题，以学习目标域具有高泛化能力的模型，如下所示：

![\<img alt="" data-attachment-key="2DVK2VWQ" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FY9Q75CLZ%22%2C%22annotationKey%22%3A%22UNT7J8I2%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B323.548%2C197%2C503.419%2C260.613%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%228%22%7D%7D" width="300" height="106" src="attachments/2DVK2VWQ.png" ztype="zimage">](attachments/2DVK2VWQ.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%228%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/CQHCXPPZ">Pan 和 Yang, 2010, p. 8</a></span>)</span>

因此，通过向每个实例 $(x_{S_i},y_{S_i})$ 添加不同的惩罚值（ $\frac{P_T(x_{T_i}, y_{T_i})}{P_S(x_{S_i}, y_{S_i})}$ ），可以学习目标域的精确模型。此外，由于 $P(Y_T|X_T)=P(Y_S|X_S)$ 。因此，$P(D_S)$ 和 $P(D_T)$ 之间的差异是由 $P(X_S)$ 和 $P(X_T)$ 造成的。并且：

![\<img alt="" data-attachment-key="PFY543QB" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FY9Q75CLZ%22%2C%22annotationKey%22%3A%22VCDEJTRB%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B364.129%2C93.355%2C458.452%2C126.258%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%228%22%7D%7D" width="157" height="55" src="attachments/PFY543QB.png" ztype="zimage">](attachments/PFY543QB.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%228%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/CQHCXPPZ">Pan 和 Yang, 2010, p. 8</a></span>)</span>

如果可以估计每个实例的 $\frac{P(x_{S_i})}{P(x_{T_i})}$ ，就可以解决Transductive迁移学习问题。

一种方法是核均值匹配（KMM）算法：通过在再生核希尔伯特空间（RKHS）中匹配源域数据和目标域数据之间的均值，直接学习 $\frac{P(x_{S_i})}{P(x_{T_i})}$ 。KMM可以重写为以下二次规划（QP）优化问题：

![\<img alt="" data-attachment-key="WEGCEVS3" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FY9Q75CLZ%22%2C%22annotationKey%22%3A%226QK5NN5K%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%229%22%2C%22position%22%3A%7B%22pageIndex%22%3A8%2C%22rects%22%3A%5B%5B66.7935500606414%2C503%2C233.613%2C614.1032291535408%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%229%22%7D%7D" width="278" height="185" src="attachments/WEGCEVS3.png" ztype="zimage">](attachments/WEGCEVS3.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%229%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/CQHCXPPZ">Pan 和 Yang, 2010, p. 9</a></span>)</span>

其中 $K_{ij}=k(x_i,x_j)$ ， $K_{S,S},K_{T,T}$ 分别是源域数据和目标域数据的核矩阵。 $\kappa_i=\frac{n_S}{n_T} \sum_{j=1}^{n_T} k\left(x_i, x_{T_j}\right)$ ，其中 $x_i\in X_S\cup X_T,x_{T_j}\in X_T$ 。可证明 $\beta_i=\frac{P(x_{S_i})}{P(x_{T_i})}$ 。

使用KMM的一个优点是，它可以避免在数据集很小时的困难执行： $P(x_{S_i})$ 或 $P(x_{T_i})$ 的密度估计。

#### 迁移特征表示的知识

一种结构对应学习（SCL）算法：利用来自目标域的未标记数据来提取一些可以减少域之间差异的相关特征。

1.  在两域中的无标签数据上定义一组（ $m$ 个）枢纽特征（特定于领域，并且依赖先验知识）。

2.  在数据上移除这些枢纽特征并将每一个枢纽特征看作新标签向量，可以构造m分类问题，假设每个问题都可以通过线性分类器解决\[ $f_l(x)=sgn(w_l^T\cdot x),l=1,\dots,m$ ]。SCL可以学习参数矩阵 $W=[w_1w_2\dots w_m]$ 。

3.  将奇异值分解（SVD）应用于矩阵 $W=[w_1w_2\dots w_m]$ 。设
$W=UDV^T$ ，然后 $\theta=U^T_{[1:h,:]}$ （ $h$ 是共享特征的数量）是行为 $W$ 的左上奇异向量的矩阵（线性映射）。

4.  标准的判别算法可以应用于扩展特征向量以建立模型。扩展特征向量包含所有原始特征 $x_i$ ，并附加新的共享特征 $\theta x_i$ 。

如果枢轴特征设计良好，则学习的映射 $\theta$ 编码了来自不同域的特征之间的对应关系。如何选择枢轴特征是困难的，并且依赖于领域。

作者提出了通过降维进行迁移学习。在这项工作中，利用了最大平均差异嵌入（MMDE）方法，最初设计用于降维，学习低维空间以减少不同域之间的分布差异，从而进行transductive迁移学习。然而，MMDE可能遭受其计算负担。因此，作者进一步提出了一种高效的特征提取算法，称为迁移成分分析（TCA），以克服MMDE的缺点。

### 无监督无迁移学习

**定义：**给定源域 $\mathcal{D}_S$ 和学习任务 $\mathcal{T}_S$ 、目标域 $\mathcal{D}_T$ 和学习任务 $\mathcal{T}_T$ ，无监督迁移学习旨在利用 $\mathcal{D}_S$ 和 $\mathcal{T}_S$ 中的知识帮助改进 $\mathcal{D}_T$ 中的目标预测函数 $f_T(\cdot)$ 的学习，其中 $\mathcal{T}_S\neq\mathcal{T}_T$ 。并且 $\mathcal{Y}_S,\mathcal{Y}_T$ 是观测不到的。

#### 迁移特征表示的知识

##### self-taught聚类（STC）

旨在借助源域中的大量未标记数据，对目标域中的少量未标记数据进行聚类。试图学习跨域的公共特征空间，这有助于在目标域中进行聚类。

目标函数如下：

![\<img alt="" data-attachment-key="C9VV7BBE" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FY9Q75CLZ%22%2C%22annotationKey%22%3A%228F2U3K75%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2210%22%2C%22position%22%3A%7B%22pageIndex%22%3A9%2C%22rects%22%3A%5B%5B34.704%2C268.137%2C252.292%2C303.942%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%2210%22%7D%7D" width="363" height="60" src="attachments/C9VV7BBE.png" ztype="zimage">](attachments/C9VV7BBE.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%2210%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/CQHCXPPZ">Pan 和 Yang, 2010, p. 10</a></span>)</span>

其中 $X_S$ 和 $X_T$ 分别是源域和目标域的数据， $Z$ 是 $X_S$ 和 $X_T$ 共享的特征空间， $I(\cdot,\cdot)$ 是两个随机变量之间的互信息。

假设存在三个聚类函数 $C_{X_T}:X_T\rightarrow \tilde{X_T},C_{X_S}:X_S\rightarrow\tilde{X_S},C_Z:Z\rightarrow\tilde{Z}$ ，其中 $\tilde{X_T},\tilde{X_S},\tilde{Z}$ 、分别是 $X_T,X_S,Z$ 的对应簇。

STC的目标是通过解决优化问题来学习 $\tilde{X_T}$ ：

![\<img alt="" data-attachment-key="DP85ZCIM" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FY9Q75CLZ%22%2C%22annotationKey%22%3A%22P6SBEZHH%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2210%22%2C%22position%22%3A%7B%22pageIndex%22%3A9%2C%22rects%22%3A%5B%5B104.662%2C137.584%2C195.003%2C166.229%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%2210%22%7D%7D" width="151" height="48" src="attachments/DP85ZCIM.png" ztype="zimage">](attachments/DP85ZCIM.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FCQHCXPPZ%22%5D%2C%22locator%22%3A%2210%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/CQHCXPPZ">Pan 和 Yang, 2010, p. 10</a></span>)</span>

##### 迁移降维方法（TDA）

TDA首先应用聚类方法为目标未标记数据生成伪类标签。然后对目标域数据和源域标记数据应用降维方法以降低维度，这两个步骤迭代运行，以找到目标数据的最佳子空间。

### 迁移边界与负迁移

迁移学习的能力是有上限的。

当源域数据和任务导致目标域中学习性能降低时，会发生**负迁移**。

## 结论及改进方向

未来，需要解决几个重要的研究问题。首先，如何避免**负迁移**是一个开放的问题。如前所述，许多提出的迁移学习算法假设源域和目标域在某种意义上彼此相关。然而，如果假设不成立，可能会发生负迁移，这可能会导致学习者的表现比完全不迁移更差。因此，如何确保不发生负迁移是迁移学习中的一个关键问题。为了避免负迁移学习，需要首先研究源域或任务与目标域或任务之间的**迁移能力**。基于合适的**可迁移性度量**，然后可以选择相关的源域或任务从中提取知识以学习目标任务。为了定义域和任务之间的可迁移性们还需要定义衡量域或任务之间**相似性**的标准。基于距离度量，可以对域或任务进行聚类，这可能有助于度量可迁移性。一个相关的问题是，当整个域不能用于迁移学习时，是否仍然可以将**部分域**迁移到目标域中进行有用的学习。

此外，迄今为止，大多数现有的迁移学习算法都专注于改进源域和目标域或任务之间的不同分布的泛化。在这样做时，他们假设源域和目标域之间的特征空间是相同的。然而，在许多应用中，可能希望跨具有不同特征空间的域或任务传递知识，并从多个这样的源域迁移知识。将这种类型的迁移学习称为**Heterogeneous迁移学习**。
