# 半监督学习文献综述

## 引用方式

### GB/T 7714

ZHU X. Semi-Supervised Learning Literature Survey\[R]. 1530. Computer Sciences, University of Wisconsin-Madison, 2005.

### BibTex

    @techreport{zhu05survey,
      author = "Xiaojin Zhu", 
      title = "Semi-Supervised Learning Literature Survey", 
      institution = "Computer Sciences, University of Wisconsin-Madison", 
      number = "1530", 
      year = 2005 
    }

## 概念名词

*   inductive learning: 训练集为 $D = \{x_{tr},y_{tr}\}$ ，测试集为 $x_{te}$ (未标记)

*   inductive semi-supervised learning: 训练集为 $D = \{x_{tr},y_{tr},x_{un}\}$ ， $x_{un}$ 与 $x_{te}$ 均未标记

*   transductive semi-supervised learning: 不管 $x_{te}$ ，只想知道对 $x_{un}$ (训练时见过，并利用了其特征信息)的效果怎么样

*   <span style="background-color: rgb(255, 255, 255)">transductive和inductive的区别在于我们想要预测的样本，是不是我们在训练的时候已经见（用）过</span>

*   up to: 在……的情况下

*   U: 无标签数据集

*   L: 标签数据集

## 主要思想

什么是半监督学习： $p(x)$ 产生的未标记样本和 $p(x,y)$ 产生的标记样本都用于估计 $p(y|x)$ 。

问题结构与模型假设的匹配程度决定了未标记数据的可利用性。例如：许多半监督学习方法假定决策边界要避开高密度 $p(x)$ 区域。提前发现糟糕的匹配是困难的，仍然是一个悬而未决的问题。

如何选择算法：

类别是否生成良好的聚类数据？如果是，具有生成混合模型的EM可能是一个不错的选择；

特征是否能自然地分到两个集合？如果是，可以进行co-training；

具有相似特征的两个点倾向于在同一类中？如果是，可以使用基于图的方法；

已经在使用SVM？transductive SVM是一种自然延伸；

现有的监督分类器是否复杂且难以修改？self-training是一种实用的包装方法。

半监督学习方法使用未标记的数据来修改或重新排序仅从标记数据获得的假设。虽然并非所有方法都是概率性的，但通过 $p(y|x)$ 表示假设，通过 $p(x)$ 表示未标记数据的方法更容易观察。生成模型具有联合分布 $p(x,y)$的共同参数。很容易看出 $p(x)$ 影响 $p(y|x)$ 。原始的辨别模型不能用于半监督学习，因为 $p(y|x)$ 被估计时忽略 $p(x)$ ，为了解决这个问题， $p(x)$ 相关项经常被引入到目标函数中，这相当于假设 $p(y|x)$ 和 $p(x)$ 共享参数。生成模型与判别模型的区别见下：

<https://www.zhihu.com/question/20446337/answer/256466823>

## 主要内容

### 生成式模型

模型假设 $p(x,y)＝p(y)p(x|y)$ ，其中 $p(x|y)$ 是可识别的混合分布，例如高斯混合模型。利用大量未标记的数据，可识别混合成分，那么理想情况下，仅需每个成分有一个标记实例来完全确定混合分布。

![\<img alt="" data-attachment-key="AXKNA2P7" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FDDANMETH%22%2C%22annotationKey%22%3A%22LSR3BHWC%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B129.182%2C209.873%2C547.409%2C613.567%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%228%22%7D%7D" width="697" height="673" src="attachments/AXKNA2P7.png" ztype="zimage">](attachments/AXKNA2P7.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%228%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/MRS6TDHG">Zhu, 2005, p. 8</a></span>)</span>

#### 可识别性

设 $\{p_θ\}$ 是由参数向量 $θ$ 索引的分布族。如果 $\theta$ 是可识别的当且仅当在不考虑混合物成分排列组合的情况下，若 $θ_1\neq θ_2$ ，有 $p_{θ_1}\neq p_{θ_2}$ 。如果模型族是可识别的，那么理论上，在不考虑成分索引的排列组合的情况下，从无限的未标记数据集 $U$ 中可学习到真实的 $\theta$ 。

可参考[Identification Theory (识别理论) 引子 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/333389952)。

举例：当有大量无标签数据时，我们已知 $p(x)$ 是\[0,1]的均匀分布，那么

$\theta:p(x|y=1),p(x|y=-1)$

$\theta_1:p(x|y=1)=unif(0,0.2),p(x|y=-1)=unif(0.2,1)$

$\theta_2: p(x|y=1)=unif(0,0.6),p(x|y=-1)=unif(0.6,1)$

经过适当的线性组合后，可使得 $p_{\theta_1}$ 和 $p_{\theta_2}$ 相等且等于已知的 $p(x)$ ，此时称 $\theta$ 是不可识别的，例如对于某个 $x=0.5$ ，无法判断其标签的值。

#### 模型正确性

如果混合模型假设正确，未标记的数据将确保提高准确性，但如果模型错误，未标记的数据实际上可能会损害准确性，例如较高的可能性可能导致较低的分类精度：

![\<img alt="" data-attachment-key="B28657VA" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FDDANMETH%22%2C%22annotationKey%22%3A%22G9TKGQGK%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2210%22%2C%22position%22%3A%7B%22pageIndex%22%3A9%2C%22rects%22%3A%5B%5B130.227%2C581.318%2C487.5%2C669.955%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2210%22%7D%7D" width="595" height="148" src="attachments/B28657VA.png" ztype="zimage">](attachments/B28657VA.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2210%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/MRS6TDHG">Zhu, 2005, p. 10</a></span>)</span>

（a）显然不是由两个高斯函数生成的。如果我们坚持每个类都是一个单一的高斯，那么（b）的概率将高于（c），但（b）的准确率约为50%，而（c）的准确度要好得多。

#### EM局部极大值

即使混合模型假设是正确的，在实践中，混合成分也通过 期望最大化（EM）算法来识别。EM易于出现局部最大值，如果局部最大值与全局最大值相差甚远，未标记的数据可能会再次影响学习，补救措施包括通过主动学习有技巧的地选择起点。

#### 聚类与标签

一些方法使用各种聚类算法来聚类整个数据集，然后用标记数据标记每个聚类，而不是使用概率生成混合模型。尽管若特定的聚类算法与真实的数据分布相匹配，它们可以表现得很好，但由于它们的算法性质，这些方法很难分析。

#### 判别学习的Fisher核

使用生成模型进行半监督学习的另一种方法是将数据转换为由生成模型确定的特征表示，然后新的特征表示被馈送到标准判别分类器中。

首先训练生成混合模型，每个类一个成分，在此阶段，可以通过EM合并未标记的数据，这与之前小节相同。然而，不是直接使用生成模型进行分类，而是将每个标记示例转换为固定长度的Fisher得分向量，即所有成分模型的对数似然关于模型参数的导数。然后，这些Fisher得分向量被用于如SVM的判别分类器中，该分类器在经验上具有高精度。

### 自训练

自训练是半监督学习的常用技术。在自训练中，首先用少量标记数据训练分类器，然后使用分类器对未标记的数据进行分类。通常，最高置信度的未标记点及其预测的标签被添加到训练集，分类器被重新训练并重复该过程。

第2节的生成模型和EM方法可以被视为“软”自训练的一个特例。

可以想象，错误的分类会使自己变强。如果预测置信度下降到阈值以下，一些算法试图通过“不学习”未标记的点来避免这种情况。

### 协同训练及多视图训练

#### 协同训练

假设：

特征可以分为两组；

每个子特征集足以训练好的分类器；

给定类，这两个集合是条件独立的。

首先，分别在两个子特征集上用标记数据训练两个单独的分类器。然后，每个分类器对未标记的数据进行分类，并用各自觉得置信度最高的少数未标记示例（和预测的标签）“教导”另一个分类器，每一个分类器都用另一个分类器给出的额外训练示例进行再训练，并重复该过程。

![\<img alt="" data-attachment-key="58C4FUB5" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FDDANMETH%22%2C%22annotationKey%22%3A%22LWL26CTN%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2212%22%2C%22position%22%3A%7B%22pageIndex%22%3A11%2C%22rects%22%3A%5B%5B209.318%2C554.727%2C402.955%2C667.909%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2212%22%7D%7D" width="323" height="189" src="attachments/58C4FUB5.png" ztype="zimage">](attachments/58C4FUB5.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2212%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/MRS6TDHG">Zhu, 2005, p. 12</a></span>)</span>

协同训练即为关于特征分割的条件独立假设。在这种假设下， $x^1$ 视图中的高置信数据点（由圆圈标记表示）将随机分散在 $x^2$ 视图中。如果它们用于在 $x^2$ 视图中“教导”分类器是有帮助的。

#### 多视图训练

多视图学习模型通常不需要协同训练的特定假设。相反，它从相同的标记数据集训练多个假设（具有不同的归纳偏差，例如决策树、SVM等），并且需要在任意给定的未标记实例上给出相似的预测。

### 避免密集区域的变化

#### <span style="background-color: rgb(255, 255, 255)">Transductive 支持向量机</span>

$p(x)$ 通常是我们从未标记数据中获得的所有信息。据信，如果 $p(x)$ 和 $p(y|x)$ 不共享参数，半监督学习无用。

TSVM通过不将边界放在高密度区域中来建立 $p(x)$  和判别决策边界之间的连接。TSVM是具有未标记数据的标准支持向量机的扩展，在标准SVM中，仅使用标记数据，目标是在再现核希尔伯特空间中找到最大边缘线性边界。在TSVM中，还使用未标记的数据，目标是找到未标记数据的标记，以便线性边界在原始标记数据和（现在已标记的）未标记数据上都具有最大边距，决策边界在未标记数据上具有最小的泛化误差界限。直观地说，未标记的数据引导线性边界远离密集区域。

TSVM可以被视为附带在未标记数据上具有额外正则化项的SVM。不妨设 $f(x)=h(x)+b$ ，其中 $h\in \mathcal{H}_K$ ( $\mathcal{H_K}$ 表示再生核希尔伯特空间)，那么优化问题为：

$\min _f \sum_{i=1}^l(1-y_i f(x_i))_{+}+\lambda_1\|h\|_{\mathcal{H}_K}^2+\lambda_2 \sum_{i=l+1}^n\max(1-|f(x_i)|,0)$

第一项为SVM项；第二项来自将标签 $sign(f(x))$ 分配给未标记的点 $x$ ，因此，未标记点上的边距为 $sign(f(x))f(x)=|f(x)|$ 。

#### 高斯过程

与标准高斯过程的关键区别在于噪声模型。“空类别噪声模型”将隐藏的连续变量 $f$ 映射到三个而不是两个标签，特别是当 $f$ 为零时，映射到从未使用过的标签“0”。此外，还限制了未标记的数据点不能使用标签0。对于未标记的点，这会使得 $f$ 的后验远离零。它实现了TSVM的类似效果，其中边缘避免了密集的未标记数据区域。然而，对过程模型没有做任何特别的处理。因此，未标记数据的所有益处都来自噪声模型。

#### 信息正则化

信息正则化通过 $p(x)$ 控制标签条件 $p(y|x)$ ，其中 $p(x)$ 可以从未标记的数据中估计。其思想是在 $p(x)$ 较高的区域，标签不应变化太大。作者使用 $x$ 和 $y$ 之间的互信息 $I(x;y)$ 作为标签复杂性的度量。当标签相似时， $I(x;y)$ 小，当标签不同时， $I$ 大。这促使在具有 $I(x;y)$ 的区域中 $p(x)$ 密度的乘积最小化（通过方差项归一化），最小化是在覆盖数据空间的多个重叠区域上执行的。

#### 熵最小化

使用未标记数据的标签的熵作为正则化因子。通过最小化熵，该方法假设先验倾向于最小化类重叠。

### 基于图的方法

基于图的半监督方法定义了一个图，其中节点是数据集中标记和未标记的示例，边（可以加权）反映了示例的相似性。这些方法通常假设图上的标签平滑性\[<span style="background-color: rgb(255, 255, 255)">如果两个点在高密度区域中（数据分布的概率密度比较大），且这两个点距离很近，那么他们的输出也会十分的接近</span>]。图方法本质上是非参数的、判别性的的和transductive的。

#### 通过图进行正则化

许多基于图的方法可以被视为估计图上的函数 $f$ 。希望 $f$ 同时满足两个条件：1）它应该接近有标记节点上的给定标签 $y_L$ 2）它应该在整个图上是平滑的。

这可以在正则化框架中表达，其中1)表示损失函数，2)表示正则化项。

几种基于图形的方法在损失函数和正则化项的特定选择上有所不同，但构造一个好的图比在方法选择更重要。

##### 最小割

在二分类情况下，正标签充当源，负标签充当汇。目标是找到一组最小的边集合，其移除会阻止所有从源到汇的流动。然后，连接到源的节点被标记为正，而连接到汇的节点则被标记为负。其目标函数为（第一项的无穷表示有标签数据被固定了， $y_i\in\{0,1\}$ ）：

$\infty \sum_{i \in L}(y_i-y_{i \mid L})^2+\frac{1}{2} \sum_{i, j} w_{i j}(y_i-y_j)^2$

最小割应用于多个扰动图，标签由多数投票决定。这个过程类似于bagging，得出“软”的最小割。

##### <span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FDDANMETH%22%2C%22pageLabel%22%3A%2219%22%2C%22position%22%3A%7B%22pageIndex%22%3A18%2C%22rects%22%3A%5B%5B158.526%2C433.404%2C415.462%2C444.313%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2219%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/DDANMETH?page=19">“Discrete Markov Random Fields: Boltzmann Machines”</a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2219%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/MRS6TDHG">Zhu, 2005, p. 19</a></span>)</span>

##### 高斯随机场与调和函数

![\<img alt="" data-attachment-key="D4BVGRNL" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FDDANMETH%22%2C%22annotationKey%22%3A%2242FN8D6G%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2219%22%2C%22position%22%3A%7B%22pageIndex%22%3A18%2C%22rects%22%3A%5B%5B197.045%2C111.545%2C424.091%2C181.773%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2219%22%7D%7D" width="378" height="117" src="attachments/D4BVGRNL.png" ztype="zimage">](attachments/D4BVGRNL.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2219%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/MRS6TDHG">Zhu, 2005, p. 19</a></span>)</span>

$\Delta$ 是图的组合拉普拉斯算子。注意 $f_i\in\mathbb{R}$ ，这是最小割的关键松弛，提供了节点边际概率的简单闭式解可能。平均值被称为调和函数。

##### 局部和全局一致性

损失函数为： $\sum_{i=1}^n(f_i-y_i)^2$

正则化项为：（引入归一化拉普拉斯算子： $D^{-\frac{1}{2}}\Delta D^{-\frac{1}{2}}=I-D^{-\frac{1}{2}}WD^{-\frac{1}{2}}$ ）

![\<img alt="" data-attachment-key="2L6AAJVA" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FDDANMETH%22%2C%22annotationKey%22%3A%22YNXHTTMA%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2220%22%2C%22position%22%3A%7B%22pageIndex%22%3A19%2C%22rects%22%3A%5B%5B160.227%2C379.5%2C450.682%2C419.045%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2220%22%7D%7D" width="484" height="66" src="attachments/2L6AAJVA.png" ztype="zimage">](attachments/2L6AAJVA.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2220%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/MRS6TDHG">Zhu, 2005, p. 20</a></span>)</span>

##### Tikhonov规范化

目标函数为（其中 $S=\Delta or \Delta^p$ $p$ 为某实数）：

![\<img alt="" data-attachment-key="WCAVG72B" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FDDANMETH%22%2C%22annotationKey%22%3A%22W3VQ5M2Z%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2220%22%2C%22position%22%3A%7B%22pageIndex%22%3A19%2C%22rects%22%3A%5B%5B236.591%2C292.909%2C372.955%2C325.636%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2220%22%7D%7D" width="227" height="54" src="attachments/WCAVG72B.png" ztype="zimage">](attachments/WCAVG72B.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2220%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/MRS6TDHG">Zhu, 2005, p. 20</a></span>)</span>

##### 流型正则化

目标函数为（ $V$ 是任意的损失函数； $K$ 是“基本核”，例如线性核或RBF核）：

![\<img alt="" data-attachment-key="RTLVCU2L" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FDDANMETH%22%2C%22annotationKey%22%3A%229AQN5DQK%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2220%22%2C%22position%22%3A%7B%22pageIndex%22%3A19%2C%22rects%22%3A%5B%5B212.727%2C164.045%2C400.227%2C209.045%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2220%22%7D%7D" width="312" height="75" src="attachments/RTLVCU2L.png" ztype="zimage">](attachments/RTLVCU2L.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2220%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/MRS6TDHG">Zhu, 2005, p. 20</a></span>)</span>

$I$ 是由标记和未标记数据导出的正则化项，例如可用（其中 $\hat{f}$ 是 $L\cup U$ 上 $f$ 求值的向量）：

![\<img alt="" data-attachment-key="EE3PQH2V" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FDDANMETH%22%2C%22annotationKey%22%3A%22ZVJNNFWV%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2221%22%2C%22position%22%3A%7B%22pageIndex%22%3A20%2C%22rects%22%3A%5B%5B244.773%2C620.182%2C363.409%2C648.818%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2221%22%7D%7D" width="198" height="48" src="attachments/EE3PQH2V.png" ztype="zimage">](attachments/EE3PQH2V.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2221%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/MRS6TDHG">Zhu, 2005, p. 21</a></span>)</span>

##### 拉普拉斯谱的图核

对于核方法，正则化项是RKHS范数 $||f||_K=f^TK^{-1}f$ ( $K$ 为核)的函数（通常单调递增）。这些核是从图中导出的，例如拉普拉斯算子。

拉普拉斯的谱变换是核适合半监督学习的原因：

扩散核对应的拉普拉斯谱变换是 $r(\lambda)=exp(-\frac{\sigma^2}{2}\lambda)$

正则化高斯过程核 $\Delta+\frac{I}{\sigma^2}$ 对应 $r(\lambda)=\frac{1}{\lambda+\sigma}$

学习图核的最优特征值实际上是（至少部分）改进不完美图的一种方法，从这个意义上说，其与图形构造有关。

##### 谱图Transducer

目标函数为（ $r_i$ 对于正标签数据为 $\sqrt{\frac{l_{-}}{l_{+}}}$ ，对于负标签数据为 $-\sqrt{\frac{l_{+}}{l_{-}}}$ ， $l_{-}$ 是负标签数据的数量； $L$ 可以是组合的或归一化的拉普拉斯图，其具有变换的谱； $c$ 是加权因子； $C$ 是错误分类损失的对角矩阵）：

![\<img alt="" data-attachment-key="SLGD43SX" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FDDANMETH%22%2C%22annotationKey%22%3A%22NWNDJNKR%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2222%22%2C%22position%22%3A%7B%22pageIndex%22%3A21%2C%22rects%22%3A%5B%5B236.591%2C458.591%2C393.409%2C494.045%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2222%22%7D%7D" width="261" height="59" src="attachments/SLGD43SX.png" ztype="zimage">](attachments/SLGD43SX.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2222%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/MRS6TDHG">Zhu, 2005, p. 22</a></span>)</span>

##### 局部学习规范化

基于图的方法的解通常可以被视为局部平均。例如，如果我们使用非归一化拉普拉斯算子，则调和函数解满足平均性质：

![\<img alt="" data-attachment-key="EIZ2N77N" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FDDANMETH%22%2C%22annotationKey%22%3A%22KF6UUR7R%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2222%22%2C%22position%22%3A%7B%22pageIndex%22%3A21%2C%22rects%22%3A%5B%5B235.909%2C219.273%2C351.818%2C259.5%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2222%22%7D%7D" width="193" height="67" src="attachments/EIZ2N77N.png" ztype="zimage">](attachments/EIZ2N77N.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2222%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/MRS6TDHG">Zhu, 2005, p. 22</a></span>)</span>

换句话说，未标记点 $x_i$ 的解 $f(x_i)$ 是其相邻解的加权平均值。注意，邻居通常也是未标记的点，因此这是一个自洽的属性。

如果将局部平均扩展到局部线性拟合，则可获得更一般的自洽性质。也就是说，可以从 $x_i$ 的两个邻居建立一个局部线性模型，并使用该线性模型预测 $x_i$ 的值。将解 $f(x_i)$ 正则化为接近此预测值。注意，将有 $n$ 个不同的线性模型，每个 $x_i$ 一个。

##### 基于树的贝叶斯

用标记和未标记的数据作为叶节点构造树 $T$ ，标记的数据被固定。作者假设了一个突变过程，其中根节点的标签向下传播到叶节点，标签沿着边向下移动时，以恒定的速率突变。结果树 $T$ （其结构和边长）唯一地定义了标签先验 $P(Y|T)$ 。在先验下，如果树中的两个叶节点更近，则它们共享相同标签的概率更高。

树的叶节点是已标记和未标记的数据，而内部节点与”物理数据“不对应。这与其他基于图的方法形成对比，其中标记和未标记的数据都是节点。

#### 图的构造

*   利用域知识
*   邻接图：例如KNN方法
*   局部拟合：类似局部线性嵌入（LLE）

#### <span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FDDANMETH%22%2C%22pageLabel%22%3A%2225%22%2C%22position%22%3A%7B%22pageIndex%22%3A24%2C%22rects%22%3A%5B%5B152.694%2C648.269%2C245.047%2C660.224%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2225%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/DDANMETH?page=25">“Fast Computation”</a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2225%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/MRS6TDHG">Zhu, 2005, p. 25</a></span>)</span>

许多半监督学习方法的规模与最初提出的 $O(n^3)$ 一样大。因为当未标记数据的容量很大时，半监督学习很有趣，这显然是一个问题。

#### 归纳

大多数基于图的半监督学习算法是transductive的，即它们不能轻易扩展到 $L\cup U$ 之外的新测试点。最近，归纳方法越来越受到关注，一种常见的做法是将图“冻结”在 $L\cup U$ 上，新点不会（尽管它们应该）改变图形结构，这避免了每次遇到新点时昂贵的图计算。

一种方法是，对于新测试点 $x$ ，对其分类方法是：

![\<img alt="" data-attachment-key="BLGY74ZI" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FDDANMETH%22%2C%22annotationKey%22%3A%22Y5F7KBTN%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2226%22%2C%22position%22%3A%7B%22pageIndex%22%3A25%2C%22rects%22%3A%5B%5B244.091%2C232.227%2C368.182%2C267.682%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2226%22%7D%7D" width="207" height="59" src="attachments/BLGY74ZI.png" ztype="zimage">](attachments/BLGY74ZI.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/MRS6TDHG">Zhu, 2005, p. 26</a></span>)</span>

#### 一致性

基于图的半监督学习算法的一致性是一个开放的研究领域。一致性的意思是当标记和未标记数据的数量增长到无穷大时，分类是否收敛到正确的解。

#### 相异边，有向图和超图

到目前为止，一个图编码了标签相似性。也就是说，如果我们希望两个示例具有相同的标签，那么它们是连接的。此外，如果边被加权，则更大的权重意味着两个节点更有可能具有相同的标签，权重总是非负的。然而，有时我们也可能有不同的信息，即两个节点应该具有不同的标签。在一般情况下，可以在同一个图上同时具有相似性和不相似性信息。

简单地用负边权重来编码不相似性是不合适的：会使目标变得非凸。

一个方法是：为不同边定义不同的图目标函数。特别是，如果 $x_i$ 和 $x_j$ 不同，则最小化 $w_{ij}(f(x_i)+f(x_j))^2$。注意，与相似边的本质区别是加号而不是减号，并且 $w_{ij}$ 保持非负值，这导致 $f(x_i)$ 和 $f(x_j)$ 具有不同的符号和相似的绝对值，因此它们相互抵消（其他相似的边也避免了平凡零解）。这样所得的目标仍然是凸的，并且可以使用线性代数轻松地求解。

对于有向图上的半监督学习，可采用hub-authority方法，从本质上将有向图转换为无向图。如果两个中心节点共同链接到重要节点，则它们通过具有适当权重的无向边连接，反之亦然。然后在无向图上进行半监督学习。

还可以使用超图来表示关系对象，其中一条边可以连接两个以上的顶点，并将谱聚类、分类和向量化(embedding)扩展到此类超图。

### 利用类别比例知识

约束未标记数据上的类比例（分类到每个类别中的实例的比例，例如20%为正，80%为负）对于半监督学习非常重要。在没有任何类比例约束的情况下，各种半监督学习算法往往会产生不平衡的输出。在极端情况下，所有未标记的数据都可能被分类到其中一个类中，这是不可取的。

一种方法是加约束：

![\<img alt="" data-attachment-key="8CFFELRM" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FDDANMETH%22%2C%22annotationKey%22%3A%22S43P9N8L%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2230%22%2C%22position%22%3A%7B%22pageIndex%22%3A29%2C%22rects%22%3A%5B%5B248.182%2C612.682%2C355.909%2C656.318%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2230%22%7D%7D" width="180" height="73" src="attachments/8CFFELRM.png" ztype="zimage">](attachments/8CFFELRM.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2230%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/MRS6TDHG">Zhu, 2005, p. 30</a></span>)</span>

注意左边的 $y$ 是预测标签，而右边的 $y$ 是已知的类（常数）。若预测值为连续，则可将 $y_i$ 表示成连续函数 $f(x_i)$ 。

类比例约束可与其他模型假设相结合，一种方式是：将 $\widetilde{p}$ 设为期望的类比例的多项式分布，以及设 $\widetilde{p}_\theta$ 为当前模型 $θ$ 产生的类比例。注意，后者是根据未标记的数据计算的。可将KL散度作为逻辑回归的正则化因子：

![\<img alt="" data-attachment-key="EWKEQHLL" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FDDANMETH%22%2C%22annotationKey%22%3A%22UKMQQ37D%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2230%22%2C%22position%22%3A%7B%22pageIndex%22%3A29%2C%22rects%22%3A%5B%5B211.364%2C384.273%2C389.318%2C423.818%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2230%22%7D%7D" width="297" height="66" src="attachments/EWKEQHLL.png" ztype="zimage">](attachments/EWKEQHLL.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2230%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/MRS6TDHG">Zhu, 2005, p. 30</a></span>)</span>

### 从未标记数据中学习域的有效编码

可以使用未标记的数据来学习问题域的有效特征编码，然后使用这个新特征表示标记的数据，并通过标准监督学习进行分类。

构建一个双视图特征生成框架，其中输入特征分割形成两个子集 $x=(z_1,z_2)$ 。假设给定类标签 $y$ ，这两个视图是条件独立的： $p(z_1,z_2|y)=p(z_1|y)p(z_2|y)$ 。

与联合训练不同，这些视图不被认为单独足以进行分类。而新颖之处在于对大量辅助问题的定义，这些是人工分类任务：使用一个视图 $z_2$ 来预测另一个视图 $t_m(z_1)$ 的某些功能，其中 $m$ 为不同的辅助问题索引。注意，辅助问题可以在未标记的数据上定义和训练。

特别地，可以定义线性模型 $w_m^⊤z_2$ 来拟合 $t_m(z_1)$ ，并使用所有未标记的数据来学习权重 $w_m$ 。权重向量 $w_m$ 具有与 $z_2$ 相同的维度。使用反映问题域中典型分类目标的辅助函数，可以想象权重集合 $\{w_1,\dots,w_m,\dots\}$ 中的一些维度更重要，表明 $z_2$ 中的相应维度更有用。这些维度（或线性组合）可以通过对由权重构建的矩阵进行奇异值分解来简洁地提取，并作为 $z_2$ 的新的更短表示。类似地， $z_1$ 通过交换 $z_1$ 和 $z_2$ 的角色而具有新的表示。最后，原始表示( $z_1,z_2$ )以及 $z_1$ 和 $z_2$ 的新表示被组成为实例 $x$ 的新表示。该新表示包含未标记数据和辅助问题的信息。然后使用新的表示对标记数据执行标准监督学习。在这种情况下，辅助问题的选择对于半监督学习的成功至关重要。

未标记数据不一定来自待分类类别的情况。例如，在图像分类任务中，这两类可以是大象和犀牛，而未标记的数据可以是任何自然场景。一种“self-taught learning”算法使用了未标记的数据来学习针对问题域调整的更高级别的表示。例如：如果图像最初由像素表示，则较高级别的表示可能是对应于某些语义（例如边缘）的小块。特别地，该算法找到一组基底 $b$ ，并且每个实例都是基底的稀疏加权组合，具有权重 $a$ 。

可通过最优化问题学习到 $b$ 和 $a$ :

![\<img alt="" data-attachment-key="KG5WXH43" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FDDANMETH%22%2C%22annotationKey%22%3A%22GTUUTETH%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2231%22%2C%22position%22%3A%7B%22pageIndex%22%3A30%2C%22rects%22%3A%5B%5B195%2C219.955%2C404.318%2C260.864%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2231%22%7D%7D" width="349" height="68" src="attachments/KG5WXH43.png" ztype="zimage">](attachments/KG5WXH43.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FMRS6TDHG%22%5D%2C%22locator%22%3A%2231%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/MRS6TDHG">Zhu, 2005, p. 31</a></span>)</span>

稀疏性对于“self-taught learning”很重要。一旦学习了基底，标记的实例将通过其在基底上的权重来表示。然后将监督算法应用于该新表示中的标记数据。
