# 通过迁移成分分析的域适应

## 引用方式

### GB/T 7714

<span style="background-color: rgb(255, 255, 255)">Pan S J, Tsang I W, Kwok J T, et al. Domain adaptation via transfer component analysis[J]. IEEE transactions on neural networks, 2010, 22(2): 199-210.</span>

### Bibtex

    @article{pan2010domain,
      title={Domain adaptation via transfer component analysis},
      author={Pan, Sinno Jialin and Tsang, Ivor W and Kwok, James T and Yang, Qiang},
      journal={IEEE transactions on neural networks},
      volume={22},
      number={2},
      pages={199--210},
      year={2010},
      publisher={IEEE}
    }

## 主要思想

域适应允许将来自源域的知识转移到不同但相关的目标领域。直观地说，发现跨域的良好特征表示至关重要。

本文首先提出通过一种新的学习方法，即用于域适应的迁移成分分析（TCA）来找到这样的表示。TCA尝试使用最大平均差异来学习再生核希尔伯特空间中跨域的一些迁移成分。在这些迁移成分所跨越的子空间中，数据的性质得到了保留，不同域中的数据分布彼此接近。因此，在这个子空间中有了新的表示，可以应用标准的机器学习方法来训练源域中的分类器或回归模型，以便在目标域中使用。

此外，为了揭示隐藏在源域和目标域的数据标签之间的关系中的知识，本文在半监督学习设置中扩展了TCA，该设置将标签信息编码为迁移成分学习。称此扩展为半监督TCA。

本文工作的主要贡献是，提出了一种新的降维框架，用于减少潜在空间中域之间的距离，以进行域适应。本文提出了无监督和半监督的特征提取方法，通过将数据投影到学习的传递分量上，可以显著减少域分布之间的距离。

最后，本文的方法可以处理大型数据集，且自然地引导到样本外泛化。通过在五个玩具数据集和两个真实世界应用（跨域室内WiFi定位和跨域文本分类）上的实验，验证了本文方法的有效性和效率。

## 主要内容

### 引言

域适应中的一个主要计算问题是如何减少源域数据和目标域数据的分布之间的差异。

直觉上，发现跨域的良好特征表示至关重要。良好的特征表示应该能够尽可能减少域之间分布的差异，同时保留原始数据的重要性质（例如几何性质、统计性质或侧信息），特别是对于目标域数据。

本文作者提出过一种新的降维方法，称为域的最大均值差异嵌入（MMDE）。MMDE旨在学习域下的共享潜在空间，其可以减少分布之间的距离，同时可以保持数据方差。然而，MMDE有两个主要的局限性：1）MMDE是transductive的，并且不推广到样本外模式，2）MMDE通过求解半定程序（SDP）来学习潜在空间，这在计算上很昂贵。

### 前人工作和准备工作

#### 域适应

包含两个部分，一个是特征空间 $\mathcal{X}$ ，另一个是边际概率分布 $P(X)$ ，其中 $X=\{x_1,\dots,x_n\}\in \mathcal{X}$ 。通常，如果两个域不同，则它们可能具有不同的特征空间或不同的边际概率分布。在本文中，关注只有一个源和一个目标域共享相同特征空间的设置。

假设一些标记的数据 $\mathcal{D}\_S$ 在源域中可用，而只有未标记的数据 $\mathcal{D}\_T$ 在目标域中可用。将源域数据表示为 $D_S=\{(x\_{S_1},y\_{S_1}),\dots,(x_{S_{n_1}},y_{S_{n_1}})\}$ ，其中 $x_{S_i}\in\mathcal{X}$ 是数据实例， $y_{S_i}\in\mathcal{Y}$ 是相应的类标签。将目标域数据表示为 $D_T=\{x_{T_1},\dots,x_{T_{n_2}}\}$ ，其中 $x_{T_i}\in\mathcal{X}$ 是数据实例。

设 $\mathcal{P}(X_S)$ 和 $\mathcal{Q}(X_T)$ （或简称 $\mathcal{P}$ 和 $\mathcal{Q}$ ）分别是 $X_S＝\{x_{S_i}\}$ 和 $X_T＝\{x_{T_i}\}$ 在源域和目标域的边缘分布。通常， $\mathcal{P}$ 和 $\mathcal{Q}$ 可以不同。本文的任务是预测与目标域中的输入 $x_{T_i}$ 相对应的标签 $y_{T_i}$ 。大多数领域自适应方法的关键假设是 $\mathcal{P}\neq\mathcal{Q}$ ，但 $P(Y_S|X_S)=P(Y_T|X_T)$ 。

#### 分布的希尔伯特空间嵌入

##### 最大平均差异(MMD)

给定从两个分布中提取的样本 $X=\{x_i\}$ 和 $Y=\{y_i\}$ ，存在许多可用于估计其距离的标准\[如KL散度]。

然而，这些估计中的许多是参数化的或需要中间密度估计。最近，通过在RKHS中嵌入分布设计了非参数的距离估计。MMD是基于此相应的RKHS距离来比较分布的方法。

设核诱导的特征映射为 $\phi$ 。 $\{x_1,\dots,x_{n_1}\}$ 和 $\{y_1,\dots,y_{n_1}\}$ 之间MMD的经验估计是 $\operatorname{MMD}(X, Y)=\left\|\frac{1}{n_1} \sum_{i=1}^{n_1} \phi\left(x_i\right)-\frac{1}{n_2} \sum_{i=1}^{n_2} \phi\left(y_i\right)\right\|\_{\mathcal{H}}^2$ ，其中 $||\cdot||\_{\mathcal{H}}$ 是RKHS范数。

因此，两个分布之间的距离只是RKHS中两个平均元素之间的距离。当RKHS是通用的时，当且仅当两个分布相同时，MMD将渐近接近零。

##### 希尔伯特-施密特独立准则(HSIC）

与MMD相关，HSIC是一个简单但强大的非参数标准，用于测量集合 $X$ 和 $Y$ 之间的相关性。

从相应的核矩阵可以很容易地获得（有偏差的）经验估计，因为 $HSIC(X,Y)=\frac{1}{(n-1)^2}tr(HKHK_{yy})$ ，其中 $K,K_{yy}$ 分别是在 $X$ 和 $Y$ 上定义的核矩阵， $H=I-\frac{1}{n}\textbf{11}^\top$ 是中心矩阵， $n$ 是 $X$ 和 $Y$ 中的样本数。

与MMD类似，如果RKHS是通用的，则HSIC渐近接近零，当且仅当 $X$ 和 $Y$ 是独立的。

#### 使用HSIC嵌入

在嵌入或降维中，通常希望保留局部数据几何结构，同时最大限度地将嵌入与可用的测信息（例如标签）对齐。

例如，在彩色最大方差展开（彩色MVU）中，以目标的嵌入 $K$ 上的局部距离约束的形式捕获局部几何，而通过HSIC标准测量与测信息（表示为核矩阵 $K_{yy}$ ）的对齐。数学上，这导致SDP：

![\<img alt="" data-attachment-key="P268BGGT" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FSZAY9QJK%22%2C%22annotationKey%22%3A%22W7DCJ3BM%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B72.629%2C170.992%2C278.131%2C195.764%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%223%22%7D%7D" width="343" height="41" src="attachments/P268BGGT.png" ztype="zimage">](attachments/P268BGGT.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/GB3T35QL">Pan 等, 2011, p. 3</a></span>)</span>

特别地，当没有给出测信息（即 $K_{yy}=I$ ）时，上式简化到MVU。

### TCA

如前所述，大多数领域自适应方法的关键假设是 $\mathcal{P}\neq\mathcal{Q}$ ，但 $P(Y_S|X_S)=P(Y_T|X_T)$ 。

然而在许多实际应用中，由于观测数据的噪声或动态因素，条件概率 $P(Y|X)$ 也可能跨域变化。本文使用较弱的假设，即 $\mathcal{P}\neq\mathcal{Q}$ ，但存在一个变换 $\phi$ ，使得 $P(\phi(X_S))\approx P(\phi(X_T))$ 且 $P(Y_S|\phi(X_S))\approx P(Y_T|\phi(X_T))$ 。

然后，可以将标准监督学习方法应用于映射的源域数据 $\phi(X_S)$ 以及相应的标签 $Y_S$ ，以训练用于映射的目标域数据 $\phi(X_T)$ 的模型。

一个关键问题是如何找到这种转换 $\phi$ 。由于目标域中没有标记数据， $\phi$ 不能通过直接最小化 $P(Y_S|\phi(X_S)),P(Y_T|\phi(X_T))$ 之间的距离来学习。

本文学习的 $\phi$ 使得：

1）边际分布 $P(\phi(X_S)),P(\phi(X_T))$ 之间的距离很小

2） $\phi(X_S),\phi(X_T)$ 保留了 $X_S$ 和 $X_T$ 的重要性质。

然后假设这样的 $\phi$ 满足 $P(Y_S|\phi(X_S))\approx P(Y_T|\phi(X_T))$ 。最后，使用在 $\phi(X_S)$ 和 $Y_S$ 上训练的分类器 $f$ 对 $\phi(X_T)$ 进行预测。

#### 最小化边际分布之间的距离

由MMD：

![\<img alt="" data-attachment-key="9KS2L8QY" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FSZAY9QJK%22%2C%22annotationKey%22%3A%224UAY6R96%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B309.097%2C438.98804047838075%2C560.765%2C455.8785648574056%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%223%22%7D%7D" width="419" height="28" src="attachments/9KS2L8QY.png" ztype="zimage">](attachments/9KS2L8QY.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/GB3T35QL">Pan 等, 2011, p. 3</a></span>)</span>

通过最小化这个量，可以找到期望的非线性映射 $\phi$ 。然而， $\phi$ 通常是高度非线性的，使关于 $\phi$ 的这个量最小化的直接优化可能陷入较差的局部极小值。

##### MMDE

一种基于降维的领域自适应方法，而不是显式地找到非线性变换 $\phi$ 。使用非线性映射 $\phi$ 将源域和目标域数据嵌入共享的低维潜在空间，然后通过求解SDP来学习相应的核矩阵K。不妨设嵌入空间中源域、目标域和跨域数据上定义的Gram矩阵分别为 $K_{S,S},K_{T,T},K_{S,T}$ 。

Gram矩阵：[Gram矩阵\_lilong117194的博客-CSDN博客](https://blog.csdn.net/lilong117194/article/details/78202637)

关键是通过最小化投影的源域和目标域数据之间的距离（等于MMD测量），同时最大化嵌入数据的方差来学习在所有数据上定义的核矩阵：

![\<img alt="" data-attachment-key="WHLXNN3E" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FSZAY9QJK%22%2C%22annotationKey%22%3A%228EMXIH8G%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B355.264%2C239.117%2C521.917%2C274.024%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%223%22%7D%7D" width="278" height="58" src="attachments/WHLXNN3E.png" ztype="zimage">](attachments/WHLXNN3E.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/GB3T35QL">Pan 等, 2011, p. 3</a></span>)</span>

通过核技巧，可以证明MMD距离可以写成 $tr(KL)$ ，其中 $K=[\phi(x_i)^\top\phi(x_j)]$ ，如果 $x_i,x_j\in X_S$ ，则 $L_{ij}=\frac{1}{n_1^2}$ ，如果 $x_i,x_j\in X_T$ ， $L_{ij}=-\frac{1}{n_1n_2}$ 。

MMDE的目标函数可以写成：

![\<img alt="" data-attachment-key="EK52XJQ7" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FSZAY9QJK%22%2C%22annotationKey%22%3A%22QTPETCPJ%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B324.861%2C116.2664144703744%2C538.808%2C141.15200000000002%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%223%22%7D%7D" width="357" height="42" src="attachments/EK52XJQ7.png" ztype="zimage">](attachments/EK52XJQ7.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/GB3T35QL">Pan 等, 2011, p. 3</a></span>)</span>

目标中的第一项使分布之间的距离最小化，第二项使特征空间中的方差最大化， $\lambda\geq 0$ 是一个折衷参数。

局限：它是transductive的，不能泛化为看不见的模式；由此产生的内核学习问题必须被昂贵的SDP求解；为了构建 $X^'\_S$ 和 $X'\_T$ 的低维表示，必须通过PCA对获得的 $K$ 进行进一步的后处理，可能会丢弃 $K$ 中可能有用的信息。

##### 针对不可见模式的参数化核映射

一种基于核特征提取寻找非线性映射 $\phi$ 的有效框架。避免了SDP的使用，从而避免了高计算负担。此外，学习到的内核可以推广到样本外模式。

提出了一种统一的内核学习方法，该方法利用了显式低秩表示，而不是像MMDE那样使用两步方法。

前文在所有数据上定义的核矩阵 $K$ 可以分解为 $K=(KK^{-\frac{1}{2}})(K^{-\frac{1}{2}}K)$ \[经验核映射]。

考虑使用将经验核映射特征转换到 $m$ 维空间 ( $m\ll n1+n2$ )的矩阵 $\tilde{W}\in \mathbb{R}^{(n_1+n_2)\times m}$ ，得到的核矩阵是( $W=K^{-\frac{1}{2}}\tilde{W}$ )：

![\<img alt="" data-attachment-key="L9D7GD9B" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FSZAY9QJK%22%2C%22annotationKey%22%3A%227SGSCHVA%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B75.444%2C496.416%2C271.937%2C521.189%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%224%22%7D%7D" width="327" height="41" src="attachments/L9D7GD9B.png" ztype="zimage">](attachments/L9D7GD9B.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%224%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/GB3T35QL">Pan 等, 2011, p. 4</a></span>)</span>

任意两个模式 $x_i$ 和 $x_j$ 之间的对应核计算是 $\tilde{k}(x_i,x_j)=k^\top_{x_i}WW^\top k_{x_j}$ ，其中 $k_x=[k(x_1,x),\dots,k(x_{n_1+n_2},x)]^\top\in\mathbb{R}^{n_1+n_2}$ 。因此，这个核 $\tilde{k}$ 有助于样本外核评估的参数化形式。

在使用上图式中的 $\tilde{K}$ 的定义时，两个域 $X'\_S$ 和 $X'\_T$ 的经验平均值之间的MMD距离可以改写为：

![\<img alt="" data-attachment-key="RVJHPHB7" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FSZAY9QJK%22%2C%22annotationKey%22%3A%2254QA8J38%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B55.739%2C377.056%2C281.509%2C395.073%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%224%22%7D%7D" width="376" height="30" src="attachments/RVJHPHB7.png" ztype="zimage">](attachments/RVJHPHB7.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%224%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/GB3T35QL">Pan 等, 2011, p. 4</a></span>)</span>

在最小化上式中，通常需要正则化项 $tr(W^\top W)$ 来控制 $W$ 的复杂性。该正则化项还可以避免广义特征值分解中分母的秩不足。

#### 保留性质

在域适应中，仅通过最小化 $P(\phi(X_S)),P(\phi(X_T))$ 之间的距离来学习变换 $\phi$ 可能是不够的。

下图显示了一个简单的二维示例，其中源域数据为红色，目标域数据为蓝色。对于两个域， $x_1$ 是区分正样本和负样本的辨别方向，而 $x_2$ 是具有小方差的噪声维度。通过只关注最小化 $P(\phi(X_S)),P(\phi(X_T))$ 之间的距离，可以选择噪声分量 $x_2$ ，然而这与目标监督任务完全无关。

![\<img alt="" data-attachment-key="STB5DCFY" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FSZAY9QJK%22%2C%22annotationKey%22%3A%22GN77DALF%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B315.12088144648027%2C633.9046900941346%2C444.22099999999983%2C740.8780111612919%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%224%22%7D%7D" width="215" height="178" src="attachments/STB5DCFY.png" ztype="zimage">](attachments/STB5DCFY.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%224%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/GB3T35QL">Pan 等, 2011, p. 4</a></span>)</span>

因此， $\phi$ 还应保留对目标监督学习任务有用的数据性质。一个明显的选择是最大限度地保留数据方差，如PCA和KPCA所执行的。

从前文式中 $\tilde{K}$ 注意到，数据在潜在空间中的嵌入是 $W^\top K$ ，其中第 $i$ 列 $[W^\top K]\_i$ 提供了 $x\_i$ 的嵌入坐标。因此，投影样本的方差为 $W^\top KHKW$ ，其中 $H=I_{n_1+n_2}−(\frac{1}{n_1+n_2})\bf{11}^\top$ 是中心矩阵， $\bf{1}\in \mathbb{R}^{n_1+n_2}$ 是全为1的列向量， $I_{n_1+n_2}\in \mathbb{R}^{(n_1+n_2)\times(n_1+n_2)}$ 是单位矩阵。

然而，在域适应中，仅关注数据方差也是不可取的。下图显示了一个示例，其中具有最大方差( $x_1$ )的方向不能用于减少跨域分布的距离，并且对于提高域适应的性能没有帮助。

![\<img alt="" data-attachment-key="7MJIZIQ6" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FSZAY9QJK%22%2C%22annotationKey%22%3A%22ZEL59DRN%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B444.221%2C634.355%2C563.58%2C741.328%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%224%22%7D%7D" width="199" height="178" src="attachments/7MJIZIQ6.png" ztype="zimage">](attachments/7MJIZIQ6.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%224%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/GB3T35QL">Pan 等, 2011, p. 4</a></span>)</span>

### 无监督TCA

结合前文，内核学习问题就变成(其中 $\mu\geq 0$ 是折衷参数， $I_m\in\mathbb{R}^{m\times m}$ 是单位矩阵，后文中 $I=I_m$ )：

![\<img alt="" data-attachment-key="SITR3VQZ" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FSZAY9QJK%22%2C%22annotationKey%22%3A%22MBQU3YXT%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B360.1622797905467%2C340.79779552470217%2C522.3113138291851%2C370.07470444834513%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%224%22%7D%7D" width="270" height="49" src="attachments/SITR3VQZ.png" ztype="zimage">](attachments/SITR3VQZ.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%224%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/GB3T35QL">Pan 等, 2011, p. 4</a></span>)</span>

尽管该优化问题涉及非凸范数约束 $W^\top KHKW=I$ ，但仍然可以通过以下优化问题有效地解决(6式即为上图式)：

![\<img alt="" data-attachment-key="VVWDRMGM" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FSZAY9QJK%22%2C%22annotationKey%22%3A%22D9ZXPDKX%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B309.66%2C89.917%2C568.085%2C257.696%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%224%22%7D%7D" width="431" height="280" src="attachments/VVWDRMGM.png" ztype="zimage">](attachments/VVWDRMGM.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%224%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/GB3T35QL">Pan 等, 2011, p. 4</a></span>)</span>

与核Fisher判别分析相似，(7)中 $W$ 的解是 $(KLK+\mu I)^{-1}KHK$ 的前 $m$ 个特征向量，其中 $m\leq n_1+n_2−1$ 。后来这将被称为TCA。

### SSTCA

一个良好的表示：1）减少源域数据和目标域数据的分布之间的距离，2）最小化源域中标记数据的经验误差。然而，无监督TCA在学习成分时不考虑标签信息。此外，许多现实世界应用中在高维观测的基础上存在一个内在的低维流形。流形信息的有效使用是许多半监督学习算法的重要组成部分。

本节将无监督TCA扩展到半监督学习环境：

1\)最大化标签相关性，而不是最小化经验误差。

2\)将流形结构编码到嵌入学习中，以便将标签信息从标记的（源域）数据传播到未标记的（目标域）数据。

注意，在传统的半监督学习设置中，标记的和未标记的数据来自同一域。然而这里的域适应环境中，标记的和未标记的数据来自不同的域。

#### 优化目标

三个理想性质：1）嵌入空间中源域和目标域数据之间分布的最大对齐；2） 高度相关的标签信息；3）保持局部几何形状。

##### 目标1——分布匹配

与无监督TCA一样，第一个目标是最小化嵌入空间中源域数据和目标域数据之间的MMD。

##### 目标2——标签相关性

最大化嵌入和标签之间的相关性（通过HSIC测量）。虽然源域数据已完全标记，但目标域数据未标记，本文提出最大程度地将嵌入\[由前文式中 $\tilde{K}$ 表示]与下式对齐( $\gamma\geq0$ 是平衡标签相关性和数据方差项的权衡参数。)：

![\<img alt="" data-attachment-key="9JPZFGTA" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FSZAY9QJK%22%2C%22annotationKey%22%3A%22CC2M9XYL%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B111.477%2C189.008%2C217.325%2C208.151%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%225%22%7D%7D" width="176" height="32" src="attachments/9JPZFGTA.png" ztype="zimage">](attachments/9JPZFGTA.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%225%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/GB3T35QL">Pan 等, 2011, p. 5</a></span>)</span>

如果 $i,j\leq n_1$ ，则 $[K_l]\_{ij}=k_{yy}(y_i,y_j)$ ，否则 $[K_l]\_{ij}=0$ ，其用于最大化标记数据的标签依赖性。而 $K_v=I$ ，用于最大限度地增加源域数据和目标域数据的方差。

通过在HSIC中替换 $\tilde{K}$ 和 $\tilde{K}_{yy}$ ，目标是最大化：

![\<img alt="" data-attachment-key="9SU4N3HH" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FSZAY9QJK%22%2C%22annotationKey%22%3A%22J87ZAQFG%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B61.369%2C93.295%2C266.307%2C113.564%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%225%22%7D%7D" width="342" height="34" src="attachments/9SU4N3HH.png" ztype="zimage">](attachments/9SU4N3HH.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%225%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/GB3T35QL">Pan 等, 2011, p. 5</a></span>)</span>

<span style="background-color: rgb(255, 255, 255)">如果源域中有足够多的标记数据，就可以通过HSIC更精确地估计特征和标签之间的依赖关系，可以使用大的</span> $\gamma$ <span style="background-color: rgb(255, 255, 255)">。否则，当源域中只有少量的标签数据，而目标域中有大量的无标签数据时，可以使用一个小的</span> $\gamma$ <span style="background-color: rgb(255, 255, 255)">。根据经验，简单地设置</span> $\gamma=0.5$ <span style="background-color: rgb(255, 255, 255)">在所有数据集上都很有效。</span>

##### 目标3——局部保留

通过对期望的核矩阵 $K$ 实施距离约束来保持流形的局部几何结构。

设 $\mathcal{N}=\{(x_i,x_j)\}$ 是互为k近邻的样本对集合， $d_{ij}=||x_i−x_j||$ 是原始输入空间中 $x_i,x_j$ 之间的距离。对于 $\mathcal{N}$ 中的每个 $(x_i,x_j)$ ，将在优化问题中添加约束 $K_{ii}+K_{jj}−2K_{ij}=d^2_{ij}$ 。因此，生成的SDP通常具有非常多的约束。

为了避免这个问题，利用了流形正则化项的局部保留性质。首先，如果 $x_i$ 是 $x_j$ 的k近邻之一，或反之，就构造一个具有关联 $m_{ij}=exp(\frac{−d^2_{ij}}{2\sigma^2})$ 的图。设 $M=[m_{ij}]$ 。

图的拉普拉斯矩阵是 $\mathcal{L}=D−M$ ，其中 $D$ 是 $d_{ii}=\sum_{j=1}^n m_{ij}$ 的对角矩阵。

直观地说，如果 $x_i,x_j$ 是输入空间中的邻居，那么 $x_i,x_j$ 的嵌入坐标之间的距离应该很小。注意， $\mathbb{R}^m$ 中的数据嵌入是 $W^\top K$ ，其中第 $[W^\top K]_i$ 列提供了 $x_i$ 的嵌入坐标。因此第三个目标是：

![\<img alt="" data-attachment-key="2A93FY43" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FSZAY9QJK%22%2C%22annotationKey%22%3A%22FVF3KP6Y%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B313.601%2C374.804%2C541.623%2C410.837%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%225%22%7D%7D" width="380" height="60" src="attachments/2A93FY43.png" ztype="zimage">](attachments/2A93FY43.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%225%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/GB3T35QL">Pan 等, 2011, p. 5</a></span>)</span>

#### 表达式和优化程序

结合所有三个目标，希望找到一个最大化目标2同时最小化目标1和目标3的 $W$ 。最终的优化问题可以写成( $\lambda\geq 0$ 是另一个折衷参数，并且 $n^2=(n_1+n_2)^2$ 是归一化项，之后 $\lambda=\frac{\lambda}{n^2}$ )：

![\<img alt="" data-attachment-key="6Y4ZC4IS" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FSZAY9QJK%22%2C%22annotationKey%22%3A%22Z9MFL4H5%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B318.105%2C263.8899999999999%2C567.522%2C308.36798528058864%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%225%22%7D%7D" width="416" height="74" src="attachments/6Y4ZC4IS.png" ztype="zimage">](attachments/6Y4ZC4IS.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%225%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/GB3T35QL">Pan 等, 2011, p. 5</a></span>)</span>

与无监督TCA类似，上式可以表示为：

![\<img alt="" data-attachment-key="5Z43NVYK" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FSZAY9QJK%22%2C%22annotationKey%22%3A%22P9AVF2KQ%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B305.718%2C168.74%2C561.328%2C202.521%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%225%22%7D%7D" width="426" height="56" src="attachments/5Z43NVYK.png" ztype="zimage">](attachments/5Z43NVYK.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%225%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/GB3T35QL">Pan 等, 2011, p. 5</a></span>)</span>

众所周知，其可以通过特征分解 $(K(L+\lambda\mathcal{L})K+\mu I)^{−1}KH\tilde{K}_{yy}HK$ 来求解。

无监督和半监督TCA的过程：

![\<img alt="" data-attachment-key="7GYR7AIK" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FSZAY9QJK%22%2C%22annotationKey%22%3A%22U6FB9LQI%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B46.73%2C558.911%2C301.777%2C739.076%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%226%22%7D%7D" width="425" height="300" src="attachments/7GYR7AIK.png" ztype="zimage">](attachments/7GYR7AIK.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/GB3T35QL">Pan 等, 2011, p. 6</a></span>)</span>

#### 计算问题

当要提取 $m$ 个非零特征向量时，时间复杂度为 $O(m(n_1+n_2)^2)$ 。

## 结论及改进方向

### 实验

#### 合成数据

进行实验以证明TCA/SSTCA在从二维数据学习一维潜在空间方面的有效性。

##### 仅最小化分布间距离

潜在空间中，SSA方法（一种找到源域数据和目标域数据的相同静态潜在空间的经验方法）在两域间距离上更小，但在域内正负标签分离程度上不如TCA（对于域适应无效），导致TCA准确性更高。

##### 仅最大化数据方差

PCA学习到的一维空间中映射数据的方差非常大，然而跨不同域的映射数据之间的距离仍然很大，并且正样本和负样本在潜在空间中重叠在一起，这对于域自适应是无效的。而通过TCA学习的一维空间中的映射数据的方差小于通过PCA学习的方差，但潜在空间中不同域数据之间的距离减小，并且正样本和负样本在潜在空间中更分离。

##### 标签信息

正样本和负样本在TCA学习的潜在空间中显著重叠。但随着标签信息的使用，正负样本在SSTCA学习到的潜在空间中更加分离，分类也变得更容易。

然而在一些应用中，源域数据的辨别方向可能与目标域数据的分辨方向完全不同。这种情况下，与无监督的TCA相比，对来自源域的标签信息进行编码（如SSTCA所做的）可能无助于甚至损害分类性能：与SSTCA学习的潜在空间相比，TCA学习到的潜在空间中目标域中的正负样本更加分离。

##### 流型信息

当流形结构可用时，拉普拉斯平滑确实可以帮助改进分类性能。

#### 跨域室内WiFi定位

![\<img alt="" data-attachment-key="JQMVTQ5M" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FSZAY9QJK%22%2C%22annotationKey%22%3A%22ZI9JX28L%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2210%22%2C%22position%22%3A%7B%22pageIndex%22%3A9%2C%22rects%22%3A%5B%5B45.682%2C312.682%2C570%2C744.273%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%2210%22%7D%7D" width="874" height="719" src="attachments/JQMVTQ5M.png" ztype="zimage">](attachments/JQMVTQ5M.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%2210%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/GB3T35QL">Pan 等, 2011, p. 10</a></span>)</span>

#### 跨域文本分类

![\<img alt="" data-attachment-key="HHITFI3N" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FSZAY9QJK%22%2C%22annotationKey%22%3A%22KN2EUJGV%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2211%22%2C%22position%22%3A%7B%22pageIndex%22%3A10%2C%22rects%22%3A%5B%5B67.562%2C395.636%2C543.875%2C731.757%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%2211%22%7D%7D" width="794" height="560" src="attachments/HHITFI3N.png" ztype="zimage">](attachments/HHITFI3N.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FGB3T35QL%22%5D%2C%22locator%22%3A%2211%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/GB3T35QL">Pan 等, 2011, p. 11</a></span>)</span>
