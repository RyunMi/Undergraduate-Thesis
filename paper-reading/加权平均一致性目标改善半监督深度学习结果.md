# 加权平均一致性目标改善半监督深度学习结果

## 引用方式

### GB/T 7714

<span style="background-color: rgb(255, 255, 255)">Tarvainen A, Valpola H. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results[J]. Advances in neural information processing systems, 2017, 30.</span>

### BibTex

    @article{tarvainen2017mean,
      title={Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results},
      author={Tarvainen, Antti and Valpola, Harri},
      journal={Advances in neural information processing systems},
      volume={30},
      year={2017}
    }

## 开源代码

[CuriousAI/mean-teacher: A state-of-the-art semi-supervised method for image recognition (github.com)](https://github.com/CuriousAI/mean-teacher)

## 主要思想

最近提出的时序集成在几个半监督学习基准测试中取得了最先进的结果，其保持每个训练样例的标签预测的指数移动平均值，并惩罚与此目标不一致的预测。然而，由于目标每个时期只更改一次，当学习大型数据集时，时序集成变得难以控制。

为克服这个问题，本文提出了均值教师，一种将模型权重平均化而不是标签预测的方法，作为额外的好处，均值教师提高了测试精度，能够减少比时序集成更少的标签进行培训。

不改变网络架构，均值教师在具有250个标签的SVHN上实现4.35％的错误率，优于使用1000个标签训练的时序集成。还表明，良好的网络架构对性能至关重要。将均值教师和残差网络相结合，将CIFAR-10的现有成果从10.55％提高到6.28％，并将ImageNet 2012的10％标签的结果从35.24％提高到9.11％。

## 主要内容

### 引论

当一个感知稍微改变时，人类通常仍然认为它是同一个物体。相应地，分类模型应该支持为相似数据点提供一致输出的函数。实现这一点的一种方法是将噪声添加到模型的输入中。为了让模型学习更抽象的不变性，噪声可以添加到中间表示，这一见解激发了许多正则化技术，如Dropout。

与在输入空间的零维数据点上最小化分类损失不同，正则化模型在每个数据点周围的流形上最小化损失，从而将决策边界推离标记的数据点\[下图(b)]。

由于未标记示例的分类损失未定义，仅通过噪声正则化无法助于半监督学习。为了克服这个问题， $\Gamma$ 模型评估每个数据点的带有噪声和不带噪声的情况，然后在两个预测之间应用*一致性损失*，在这种情况下，模型扮演双重角色，既是教师又是学生：作为学生，它像以前一样学习；作为教师，它制定目标，然后把这些目标作为学习的学生使用，由于模型本身会生成目标，所以它们很可能是不正确的。如果对生成的目标给予过多的权重，则不一致性的损失超过了错误分类的损失，从而阻止了新信息的学习。实际上，该模型存在确认偏差\[下图(c)]，这是一种可以通过提高目标质量来减轻的风险。

至少有两种方法可以提高目标质量。一种方法是谨慎地选择表示的扰动，而不是仅仅应用加法或乘法噪声(ViAT，并表明虚拟对抗训练可以产生令人印象深刻的结果)。另一种方法是仔细选择教师模式，而不是勉强复制学生模式(本文)。两种方法其实可以结合，本文未讨论。

本文目标是在没有额外训练的情况下，从学生模式中形成更好的教师模式。第一步，考虑模型的softmax输出通常不能在训练数据之外提供准确的预测，这可以通过在推理时向模型中添加噪声来部分缓解，因此噪声教师可以产生更准确的目标( $\Pi$ 模型，下图(d))。

$\Pi$ 模型可以通过时序集成进一步改进，其为每个训练示例保持一个指数移动平均(EMA)的预测值。在每个训练步骤中，基于新的预测结果，所有小批量示例的EMA预测都会被更新，因此每个示例的EMA预测是由模型当前版本和之前评估同一示例的版本组成的集合，这种集成提高了预测的质量，并且将它们用作教师预测可以改善结果。然而，由于每个目标每个时期只更新一次，所以学到的信息以缓慢的速度纳入训练过程中。

下图是一项二分类任务的草图，其中有两个标记示例(大蓝点)和一个未标记的示例，展示了未标记目标的选择(黑圆)如何影响适配函数(灰色曲线)。

![\<img alt="" data-attachment-key="7P36HVL8" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBJR4EIUQ%22%2C%22annotationKey%22%3A%22JHWPNGVY%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B113.834%2C632.519%2C497.039%2C715.359%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F27IXBZB7%22%5D%2C%22locator%22%3A%222%22%7D%7D" width="639" height="138" src="attachments/7P36HVL8.png" ztype="zimage">](attachments/7P36HVL8.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F27IXBZB7%22%5D%2C%22locator%22%3A%222%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/27IXBZB7">Tarvainen 和 Valpola, p. 2</a></span>)</span>

(a)没有正则化的模型可以适配任何能够很好预测已标记训练样本的功能，但容易过拟合。

(b)一个使用嘈杂标记数据(小点)训练的模型学会了在标记数据点周围给出一致的预测。

(c)对未标记示例周围噪声的一致性提供了额外的平滑。教师模型(灰色曲线)首先适应于标记示例中，然后在学生模型的训练期间保持不变。

在图d和图e中省略了小点：

(d)教师模型上的噪声在没有额外训练的情况下减少了目标的偏差。随机梯度下降的预期方向是朝向单个噪声目标(蓝色小圆圈)的平均值(蓝色大圆圈)。

(e)一种集成模型给出了更好的预期目标。时序集成法和本文都使用这种方法。

### 均值教师

为了克服时序集成的局限性，本文对模型权重进行平均，而不是将预测平均。由于教师模型是连续学生模型的平均值，称之为均值教师方法：

![\<img alt="" data-attachment-key="3TJA8QUX" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBJR4EIUQ%22%2C%22annotationKey%22%3A%22GPZVBZVQ%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B105.94475138121547%2C531.646%2C505.492%2C711.9779999999998%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F27IXBZB7%22%5D%2C%22locator%22%3A%223%22%7D%7D" width="666" height="301" src="attachments/3TJA8QUX.png" ztype="zimage">](attachments/3TJA8QUX.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F27IXBZB7%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/27IXBZB7">Tarvainen 和 Valpola, p. 3</a></span>)</span>

\[上图描述了一个训练批次，其中只有一个带标签的示例。学生模型和教师模型都在其计算过程中应用噪声 $(\eta,\eta')$ 来评估输入。通过分类损失函数将学习模型的softmax输出与独热标签进行比较，同时使用一致性损失函数将其与教师模型输出进行比较。在使用梯度下降算法更新学生模型的权重后，教师模型的权重会被更新为学生权重的指数移动平均值。这两种模型输出都可用于预测，但训练结束后，教师模型的预测更可能是正确的。使用未标记示例的训练步骤将类似，除了不会应用分类成本。]

在训练步骤中平均模型权重往往比直接使用最终权重产生更准确的模型。教师模型使用学生模型的EMA权重，而不是与学生模型共享权重。此时它可以在每一步骤而不是每一个epoch之后集成信息。由于权重平均值提高了所有层的输出，而不仅仅是顶层输出，因此目标模型具有更好的中间表示。

与时序集成相比，上述带来了两个实际优势：首先，更准确的目标标签会导致学生和教师模型之间更快的反馈循环，从而提高测试准确性。其次，该方法可扩展到大型数据集和在线学习。

更正式地，本文定义一致性代价 $J$ 为学生模型（具有权重 $\theta$ 和噪声 $\eta$ ）与教师模型（具有权重 $\theta'$ 和噪声 $\eta'$ ）预测之间的预期距离：

![\<img alt="" data-attachment-key="6Z6RKUV2" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBJR4EIUQ%22%2C%22annotationKey%22%3A%22C98XEDMS%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B214.707%2C247.624%2C397.856%2C270.729%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F27IXBZB7%22%5D%2C%22locator%22%3A%223%22%7D%7D" width="305" height="38" src="attachments/6Z6RKUV2.png" ztype="zimage">](attachments/6Z6RKUV2.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F27IXBZB7%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/27IXBZB7">Tarvainen 和 Valpola, p. 3</a></span>)</span>

时序集成和均值教师的区别在于教师预测的产生方式。而与此不同的是， $\Pi$ 模型使用的是 $\theta=\theta'$ ，而时序集成则是通过加权平均来近似 $f(x,\theta',\eta')$ 。本文将 $\theta'_t$ 定义为训练步骤 $t$ 的连续 $\theta$ 权重的EMA：

![\<img alt="" data-attachment-key="LILUUXML" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBJR4EIUQ%22%2C%22annotationKey%22%3A%22HCC2LEMU%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B254.718%2C153.514%2C356.155%2C167.039%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F27IXBZB7%22%5D%2C%22locator%22%3A%223%22%7D%7D" width="169" height="23" src="attachments/LILUUXML.png" ztype="zimage">](attachments/LILUUXML.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F27IXBZB7%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/27IXBZB7">Tarvainen 和 Valpola, p. 3</a></span>)</span>

其中 $\alpha$ 是平滑系数超参数。三种算法之间的另一个区别在于， $\Pi$ 模型将训练应用于 $\theta'$ ，而时序集成和均值教师则将其视为优化的常数。

可以通过在每个训练步骤中使用随机梯度下降采样噪声 $\eta,\eta'$ 来近似一致性成本函数 $J$ 。在大多数实验中使用均方误差(MSE)作为一致性成本。

## 结论及改进方向

### 实验

模型结构是一个具有13层卷积神经网络(ConvNet)，并包含三种类型的噪声：随机平移和水平翻转输入图像，输入层上的高斯噪声以及在网络中应用的dropout。所有比较方法都使用类似的13层ConvNet体系结构。

![\<img alt="" data-attachment-key="W2QI59GC" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBJR4EIUQ%22%2C%22annotationKey%22%3A%22SEARZWVL%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B121.724%2C554.751%2C491.403%2C673.657%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F27IXBZB7%22%5D%2C%22locator%22%3A%224%22%7D%7D" width="616" height="198" src="attachments/W2QI59GC.png" ztype="zimage">](attachments/W2QI59GC.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F27IXBZB7%22%5D%2C%22locator%22%3A%224%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/27IXBZB7">Tarvainen 和 Valpola, p. 4</a></span>)</span>

上图为SVHN结果，下图为CIFAR-10结果。

![\<img alt="" data-attachment-key="FTRCA6MY" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBJR4EIUQ%22%2C%22annotationKey%22%3A%222ZX5B5D8%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B122.287%2C392.453%2C490.276%2C507.978%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F27IXBZB7%22%5D%2C%22locator%22%3A%224%22%7D%7D" width="613" height="192" src="attachments/FTRCA6MY.png" ztype="zimage">](attachments/FTRCA6MY.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F27IXBZB7%22%5D%2C%22locator%22%3A%224%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/27IXBZB7">Tarvainen 和 Valpola, p. 4</a></span>)</span>

#### 带有额外未标记数据的SVHN

为测试方法是否已经达到了极限：

除了主要的训练数据外，SVHN还包括一个由531131个示例组成的额外数据集。本文从主要的训练数据中选择了500个样本作为我们的标记训练样本。使用剩余的原始训练集和额外的训练集作为未标记的样本

![\<img alt="" data-attachment-key="CK29T2ZU" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBJR4EIUQ%22%2C%22annotationKey%22%3A%22B8NIE5JE%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B154.97237569060772%2C625.7569999999998%2C458.155%2C682.674%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F27IXBZB7%22%5D%2C%22locator%22%3A%225%22%7D%7D" width="505" height="95" src="attachments/CK29T2ZU.png" ztype="zimage">](attachments/CK29T2ZU.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F27IXBZB7%22%5D%2C%22locator%22%3A%225%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/27IXBZB7">Tarvainen 和 Valpola, p. 5</a></span>)</span>

#### 分析训练曲线

![\<img alt="" data-attachment-key="D4ZEDN6G" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBJR4EIUQ%22%2C%22annotationKey%22%3A%22CSCVBEWA%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B109.32596685082872%2C405.978%2C504.36499999999995%2C599.834%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F27IXBZB7%22%5D%2C%22locator%22%3A%225%22%7D%7D" width="658" height="323" src="attachments/D4ZEDN6G.png" ztype="zimage">](attachments/D4ZEDN6G.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F27IXBZB7%22%5D%2C%22locator%22%3A%225%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/27IXBZB7">Tarvainen 和 Valpola, p. 5</a></span>)</span>

上图的训练曲线有利于了解使用均值教师的效果。正如预期的那样，在初始阶段之后，经过EMA加权的模型(底部一行中的蓝色和深灰色曲线)比纯学生模型(橙色和浅灰色)给出更准确的预测。

使用EMA加权模型作为教师可以改善半监督设置中的结果。看起来存在一个良性反馈循环，即教师(蓝色曲线)通过一致性成本改善学生(橙色)，学生通过指数移动平均值改善教师。如果这个反馈循环被切断，学习就会变慢，并且模型会更早地开始过拟合(深灰色和浅灰)。

当标签不足时，均值教师模型提供帮助。在使用 500 个标签(中间列图)时，均值教师模型学习速度更快，并在 $\Pi$ 模型停止改进后继续训练。另一方面，在所有标记的情况下(左列图)，均值教师模型和 $\Pi$ 模型的行为几乎相同。

均值教师使用未标记的训练数据比 $\Pi$ 模型更有效，如中间列图所示。另一方面，使用额外的50万个未标记示例(右侧列图)， $\Pi$ 模型可以持续改进更长时间。均值教师学习速度更快，并最终收敛到更好的结果，但大量数据似乎抵消了 $\Pi$ 模型的更差预测。

#### 消融实验

为了评估模型各个方面的重要性，针对仅有250个标签的SVHN数据集进行了实验，一次性改变一个或几个超参数，同时保持其他参数不变。

##### 去除噪声

通过在模型的两侧添加噪声， $\Pi$ 模型能够产生更好的预测结果，在均值教师中也一样。

![\<img alt="" data-attachment-key="JI8N9LFU" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBJR4EIUQ%22%2C%22annotationKey%22%3A%22CAFQSH7A%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B111.58%2C642.663%2C364.608%2C713.669%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F27IXBZB7%22%5D%2C%22locator%22%3A%226%22%7D%7D" width="422" height="118" src="attachments/JI8N9LFU.png" ztype="zimage">](attachments/JI8N9LFU.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F27IXBZB7%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/27IXBZB7">Tarvainen 和 Valpola, p. 6</a></span>)</span>

可以发现，要获得可接受的性能，输入扩充或dropout是必要的。另一方面，当使用扩充时，输入噪声并没有帮助。在教师端使用dropout仅比在学生端使用dropout略有益处，至少在使用输入扩充时是如此。

##### 对EMA衰减和一致性权重的敏感性

均值教师算法的基本超参数是一致性成本权重和EMA衰减。

![\<img alt="" data-attachment-key="GJIWY3X9" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBJR4EIUQ%22%2C%22annotationKey%22%3A%22D9HGYA73%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B382.077%2C640.409%2C499.856%2C717.613%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F27IXBZB7%22%5D%2C%22locator%22%3A%226%22%7D%7D" width="196" height="128" src="attachments/GJIWY3X9.png" ztype="zimage">](attachments/GJIWY3X9.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F27IXBZB7%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/27IXBZB7">Tarvainen 和 Valpola, p. 6</a></span>)</span>

![\<img alt="" data-attachment-key="4ZYSK86U" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBJR4EIUQ%22%2C%22annotationKey%22%3A%22KMH9SZAD%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B111.017%2C549.68%2C227.669%2C634.21%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F27IXBZB7%22%5D%2C%22locator%22%3A%226%22%7D%7D" width="194" height="141" src="attachments/4ZYSK86U.png" ztype="zimage">](attachments/4ZYSK86U.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F27IXBZB7%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/27IXBZB7">Tarvainen 和 Valpola, p. 6</a></span>)</span>

可以看到，在每种情况下，良好的值大致跨越一个数量级，超出这些范围，性能迅速降低。

在评估运行中，启动阶段使用了EMA衰减率 $\alpha=0.99$ ，而在其余的训练过程中使用了 $\alpha=0.999$ 。选择这种策略是因为学生在训练初期很快进步，因此老师应该迅速忘记旧的、不准确的学生权重。后来，学生的进步会变慢，老师会从更长的记忆中获益。

##### 解耦分类和一致性

教师预测的始终如一可能并不一定是分类任务的良好代理，尤其是在培训早期。

本文模型通过使用相同的输出强烈地耦合了这两个任务。将模型改为具有两个顶层，并产生两个输出以分析解耦任务。之后对其中一个输出进行了分类训练，另一个进行一致性训练。还添加了输出 logit 之间的均方误差成本，并且变化了该成本的权重，使耦合强度可控。

![\<img alt="" data-attachment-key="5IJ2ES3S" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBJR4EIUQ%22%2C%22annotationKey%22%3A%22U6NK2ZS8%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B248.519%2C550.807%2C367.989%2C633.646%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F27IXBZB7%22%5D%2C%22locator%22%3A%226%22%7D%7D" width="199" height="138" src="attachments/5IJ2ES3S.png" ztype="zimage">](attachments/5IJ2ES3S.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F27IXBZB7%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/27IXBZB7">Tarvainen 和 Valpola, p. 6</a></span>)</span>

可以看到强耦合版本表现很好，而耦合过松的版本则不行。另一方面，适度解耦似乎有助于使一致性逐渐升级变得多余。

##### 从MSE转换为KL散度

KL散度似乎比MSE更自然。使用了一个成本函数族的实例，从均方差(在图中 $\tau= 0$ )到KL散度( $\tau=1$ )：

![\<img alt="" data-attachment-key="4BJ42SD5" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBJR4EIUQ%22%2C%22annotationKey%22%3A%22NPKN2QWQ%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B382.077%2C546.862%2C501.547%2C635.337%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F27IXBZB7%22%5D%2C%22locator%22%3A%226%22%7D%7D" width="199" height="147" src="attachments/4BJ42SD5.png" ztype="zimage">](attachments/4BJ42SD5.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F27IXBZB7%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/27IXBZB7">Tarvainen 和 Valpola, p. 6</a></span>)</span>

发现在这种设置下，均方差表现比其他成本函数更好。

#### 使用残差网络在CIFAR-10和ImageNet上训练的均值教师

为了探索模型架构的效果，在CIFAR-10上使用了带有Shake-Shake正则化的12个块(26层)残差网络(ResNet)进行了实验。为测试这些方法是否适用于更自然的图像，使用Imagenet 2012数据集对10%的标签进行了实验。使用了一个50个块(152层)的ResNeXt架构，并看到了明显的技术飞跃。由于测试集不是公共可用的，使用验证集来衡量结果：

![\<img alt="" data-attachment-key="486ZLV8B" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2FBJR4EIUQ%22%2C%22annotationKey%22%3A%2235RS4SDH%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B156.663%2C588%2C456.464%2C675.912%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F27IXBZB7%22%5D%2C%22locator%22%3A%227%22%7D%7D" width="500" height="147" src="attachments/486ZLV8B.png" ztype="zimage">](attachments/486ZLV8B.png)\
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F8273795%2Fitems%2F27IXBZB7%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/27IXBZB7">Tarvainen 和 Valpola, p. 7</a></span>)</span>
